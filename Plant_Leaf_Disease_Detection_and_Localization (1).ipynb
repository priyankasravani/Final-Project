{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gIohmF0ylke"
   },
   "source": [
    "#Making Colab Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "py-egLGVyplD"
   },
   "source": [
    "First for fetching the datset so mounting drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwTwKq2si8dY"
   },
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqOxzGSVZyQv",
    "outputId": "247e27bb-ee01-4c97-9e07-6f1f0fd0f05a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttsgdWDqaKrZ"
   },
   "source": [
    "#Importing libraries then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "at3eILLmaIgD"
   },
   "outputs": [],
   "source": [
    "import sys,os,random,pprint,time,pickle,math,cv2,copy\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.objectives import categorical_crossentropy\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.utils import generic_utils\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras import initializers, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNP4-pBIahmE"
   },
   "source": [
    "Config settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQ0LOCk6aNpH"
   },
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\t# Set True if you want to see the progress updates\n",
    "\t\tself.verbose = True\n",
    "\n",
    "\t\t# Set True for data augmentation\n",
    "\t\tself.use_horizontal_flips = True\n",
    "\t\tself.use_vertical_flips = True\n",
    "\t\tself.rot_90 = True\n",
    "\n",
    "\t\t# Anchor box scales. In order to choose the set of anchors we usually define a set of sizes (e.g. 64px, 128px, 256px).\n",
    "    # Original anchor_box_scales in the paper is [128, 256, 512] \n",
    "    # If size of the image is smaller, anchor_box_scales should be scaled accordingly\n",
    "\t\tself.anchor_box_scales = [128, 256, 512] # since leaves are more than 256 pixel\n",
    "\n",
    "\t\t# Anchor box ratios, a set of ratios between width and height of boxes (e.g. 0.5, 1, 1.5) and use all the possible combinations of sizes and ratios.\n",
    "\t\tself.anchor_box_ratios = [[1, 1], [0.75, 1.5], [1.5, 0.75]]\n",
    "\n",
    "\t\t# Size to resize the smallest side of the image. Original setting in paper is 600. Smaller will save training time\n",
    "\t\tself.im_size = 256\n",
    "\n",
    "\t\t# image channel-wise mean to subtract\n",
    "\t\tself.img_channel_mean = [103.939, 116.779, 123.68]          #At the moment utilized as it is but we projected our own by running MeanStd.py on combined dataset\n",
    "\t\tself.img_scaling_factor = 1.0\n",
    "\n",
    "\t\t# number of ROIs at once\n",
    "\t\tself.num_rois = 4\n",
    "\n",
    "\t\t# stride at the RPN (this depends on the network configuration)\n",
    "\t\tself.rpn_stride = 16\n",
    "\n",
    "\t\tself.balanced_classes = False\n",
    "\n",
    "\t\t# scaling the stdev\n",
    "\t\tself.std_scaling = 4.0\n",
    "\t\tself.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n",
    "\n",
    "\t\t# overlaps for RPN\n",
    "\t\tself.rpn_min_overlap = 0.3\n",
    "\t\tself.rpn_max_overlap = 0.7\n",
    "\n",
    "\t\t# overlaps for classifier ROIs\n",
    "\t\tself.classifier_min_overlap = 0.1\n",
    "\t\tself.classifier_max_overlap = 0.5\n",
    "\n",
    "    # Just Name the base network, we chose vgg other options are like Resnet50, UNet etc can be replaced by as well\n",
    "\t\tself.network = 'vgg'\n",
    "\n",
    "\t\t# placeholder for the class mapping, automatically generated by the parser\n",
    "\t\tself.class_mapping = None\n",
    "\t\tself.model_path = None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_f0tPDJaoTV"
   },
   "source": [
    "Parser the data from annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1XM3J3-ak9p"
   },
   "outputs": [],
   "source": [
    "def get_data(input_path):\n",
    "\t\"\"\"Parse the data from annotation file\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tinput_path: annotation file path\n",
    "\n",
    "\tReturns:\n",
    "\t\tall_data: list(filepath, width, height, list(bboxes))\n",
    "\t\tclasses_count: dict{key:class_name, value:count_num} \n",
    "\t\t\te.g. {'Apple-Scab-Leaf': 158, 'Apple-leaf': 236, 'Apple-rust-leaf': 168, 'Bell_pepper-leaf': 312, 'Bell_pepper-leaf-spot': 249, 'Blueberry-leaf': 823,\n",
    "\t\tclass_mapping: dict{key:class_name, value: idx}\n",
    "\t\t\te.g. {'Apple-Scab-Leaf': 0, 'Apple-leaf': 1, 'Apple-rust-leaf': 2}\n",
    "\t\"\"\"\n",
    "\tfound_bg = False\n",
    "\tall_imgs = {}\n",
    "\n",
    "\tclasses_count = {}\n",
    "\n",
    "\tclass_mapping = {}\n",
    "\n",
    "\tvisualise = True\n",
    "\n",
    "\ti = 1\n",
    "\t\n",
    "\twith open(input_path,'r') as f:\n",
    "\n",
    "\t\tprint('Parsing annotation files')\n",
    "\n",
    "\t\tfor line in f:\n",
    "\n",
    "\t\t\t# Print process\n",
    "\t\t\tsys.stdout.write('\\r'+'idx=' + str(i))\n",
    "\t\t\ti += 1\n",
    "\n",
    "\t\t\tline_split = line.strip().split(',')\n",
    "\n",
    "\t\t\t# Make sure the info saved in annotation file matching the format (path_filename, x1, y1, x2, y2, class_name)\n",
    "\t\t\t\n",
    "\t\t\ttry:\n",
    "\t\t\t  (filename,x1,y1,x2,y2,class_name) = (line_split[0][1:],line_split[1],line_split[2],line_split[3],line_split[4],line_split[-1][:-1])#(filename,x1,y1,x2,y2,class_name) = line_split\n",
    "\t\t\texcept:\n",
    "\t\t\t  (filename,x1,y1,x2,y2,class_name) = (line_split[0],line_split[1],line_split[2],line_split[3],line_split[4],line_split[5])#(filename,x1,y1,x2,y2,class_name) = line_split\n",
    "\n",
    "\t\t\tif class_name not in classes_count:\n",
    "\t\t\t\tclasses_count[class_name] = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tclasses_count[class_name] += 1\n",
    "\n",
    "\t\t\tif class_name not in class_mapping:\n",
    "\t\t\t\tif class_name == 'bg' and found_bg == False:\n",
    "\t\t\t\t\tprint('Found class name with special name bg. Will be treated as a background region (this is usually for hard negative mining).')\n",
    "\t\t\t\t\tfound_bg = True\n",
    "\t\t\t\tclass_mapping[class_name] = len(class_mapping)\n",
    "\n",
    "\t\t\tif filename not in all_imgs:\n",
    "\t\t\t\tall_imgs[filename] = {}\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t  img = cv2.imread(filename)\n",
    "\t\t\t\t  (rows,cols) = img.shape[:2]\n",
    "\t\t\t\t  all_imgs[filename]['filepath'] = filename\n",
    "\t\t\t\t  all_imgs[filename]['width'] = cols\n",
    "\t\t\t\t  all_imgs[filename]['height'] = rows\n",
    "\t\t\t\t  all_imgs[filename]['bboxes'] = []\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t  print(\"Something is wrong with this image : \", filename)\n",
    "\t\t\t\t  \n",
    "\t\t\ttry:\n",
    "\t\t\t    all_imgs[filename]['bboxes'].append({'class': class_name, 'x1': int(x1), 'x2': int(x2), 'y1': int(y1), 'y2': int(y2)})\n",
    "\t\t\texcept:\n",
    "\t\t\t    pass\n",
    "\n",
    "\t\tall_data = []\n",
    "\t\tfor key in all_imgs:\n",
    "\t\t\tall_data.append(all_imgs[key])\n",
    "\t\t\n",
    "\t\t# make sure the bg class is last in the list\n",
    "\t\tif found_bg:\n",
    "\t\t\tif class_mapping['bg'] != len(class_mapping) - 1:\n",
    "\t\t\t\tkey_to_switch = [key for key in class_mapping.keys() if class_mapping[key] == len(class_mapping)-1][0]\n",
    "\t\t\t\tval_to_switch = class_mapping['bg']\n",
    "\t\t\t\tclass_mapping['bg'] = len(class_mapping) - 1\n",
    "\t\t\t\tclass_mapping[key_to_switch] = val_to_switch\n",
    "\t\t\n",
    "\t\treturn all_data, classes_count, class_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CCVODawxRqH"
   },
   "source": [
    "#Collecting Features and Proposing boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgzzP8g_ayCh"
   },
   "source": [
    "Vgg-16 model base network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCShJsKEaw5w"
   },
   "outputs": [],
   "source": [
    "\n",
    "#F1 Get output image lengths \n",
    "def get_img_output_length(width, height):\n",
    "    def get_output_length(input_length):\n",
    "        return input_length//16\n",
    "\n",
    "    return get_output_length(width), get_output_length(height)    \n",
    "\n",
    "\n",
    "\n",
    "def nn_base(input_tensor=None, trainable=False):\n",
    "\n",
    "    input_shape = (None, None, 3)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)            # Getting image shape\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape) #Validating tensor input\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    bn_axis = 3\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjOB3ocBa_Ly"
   },
   "source": [
    "RPN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pC9MEuNFa5mo"
   },
   "outputs": [],
   "source": [
    "def rpn_layer(base_layers, num_anchors):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        base_layers: vgg in here\n",
    "        num_anchors: 9 in here\n",
    "\n",
    "    Returns:\n",
    "        [x_class, x_regr, base_layers]\n",
    "        x_class: classification for whether it's an object\n",
    "        x_regr: bboxes regression\n",
    "        base_layers: vgg in here\n",
    "    \"\"\"\n",
    "    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n",
    "\n",
    "    x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n",
    "    x_regr  = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n",
    "\n",
    "    return [x_class, x_regr, base_layers]\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dnx39FSKkPM6"
   },
   "source": [
    "#Interim functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBrgPAEfbGA5"
   },
   "outputs": [],
   "source": [
    "def union(au, bu, area_intersection):\n",
    "\tarea_a = (au[2] - au[0]) * (au[3] - au[1])\n",
    "\tarea_b = (bu[2] - bu[0]) * (bu[3] - bu[1])\n",
    "\tarea_union = area_a + area_b - area_intersection\n",
    "\treturn area_union\n",
    "\n",
    "\n",
    "def intersection(ai, bi):\n",
    "\tx = max(ai[0], bi[0])\n",
    "\ty = max(ai[1], bi[1])\n",
    "\tw = min(ai[2], bi[2]) - x\n",
    "\th = min(ai[3], bi[3]) - y\n",
    "\tif w < 0 or h < 0:\n",
    "\t\treturn 0\n",
    "\treturn w*h\n",
    "\n",
    "\n",
    "def iou(a, b):\n",
    "\t# a and b should be (x1,y1,x2,y2)\n",
    "\n",
    "\tif a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]: #making sure the flaw of min max is not there..\n",
    "\t\treturn 0.0                                                      #if flaw is traced then immediately return zero\n",
    "\n",
    "\tarea_i = intersection(a, b)\n",
    "\tarea_u = union(a, b, area_i)\n",
    "\n",
    "\treturn float(area_i) / float(area_u + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDfIK1iTejja"
   },
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    np.testing.assert_array_less(x1, x2)\n",
    "    np.testing.assert_array_less(y1, y2)\n",
    "\n",
    "    # if the bounding boxes integers, convert them to floats --\n",
    "    # this is important since we'll be doing a bunch of divisions\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "\n",
    "    # initialize the list of picked indexes\t\n",
    "    pick = []\n",
    "\n",
    "    # calculate the areas\n",
    "    area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "    # sort the bounding boxes \n",
    "    idxs = np.argsort(probs)\n",
    "\n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list and add the\n",
    "        # index value to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "\n",
    "        # find the intersection\n",
    "\n",
    "        xx1_int = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1_int = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2_int = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2_int = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "        ww_int = np.maximum(0, xx2_int - xx1_int)\n",
    "        hh_int = np.maximum(0, yy2_int - yy1_int)\n",
    "\n",
    "        area_int = ww_int * hh_int\n",
    "\n",
    "        # find the union\n",
    "        area_union = area[i] + area[idxs[:last]] - area_int\n",
    "\n",
    "        # compute the ratio of overlap\n",
    "        overlap = area_int/(area_union + 1e-6)\n",
    "\n",
    "        # delete all indexes from the index list that have\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlap_thresh)[0])))\n",
    "\n",
    "        if len(pick) >= max_boxes:\n",
    "            break\n",
    "\n",
    "    # return only the bounding boxes that were picked using the integer data type\n",
    "    boxes = boxes[pick].astype(\"int\")\n",
    "    probs = probs[pick]\n",
    "    return boxes, probs\n",
    "\n",
    "def apply_regr_np(X, T):\n",
    "    \"\"\"Apply regression layer to all anchors in one feature map\n",
    "\n",
    "    Args:\n",
    "        X: shape=(4, 18, 25) the current anchor type for all points in the feature map\n",
    "        T: regression layer shape=(4, 18, 25)\n",
    "\n",
    "    Returns:\n",
    "        X: regressed position and size for current anchor\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x = X[0, :, :]\n",
    "        y = X[1, :, :]\n",
    "        w = X[2, :, :]\n",
    "        h = X[3, :, :]\n",
    "\n",
    "        tx = T[0, :, :]\n",
    "        ty = T[1, :, :]\n",
    "        tw = T[2, :, :]\n",
    "        th = T[3, :, :]\n",
    "\n",
    "        cx = x + w/2.\n",
    "        cy = y + h/2.\n",
    "        cx1 = tx * w + cx\n",
    "        cy1 = ty * h + cy\n",
    "\n",
    "        w1 = np.exp(tw.astype(np.float64)) * w\n",
    "        h1 = np.exp(th.astype(np.float64)) * h\n",
    "        x1 = cx1 - w1/2.\n",
    "        y1 = cy1 - h1/2.\n",
    "\n",
    "        x1 = np.round(x1)\n",
    "        y1 = np.round(y1)\n",
    "        w1 = np.round(w1)\n",
    "        h1 = np.round(h1)\n",
    "        return np.stack([x1, y1, w1, h1])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return X\n",
    "    \n",
    "def apply_regr(x, y, w, h, tx, ty, tw, th):\n",
    "    # Apply regression to x, y, w and h\n",
    "    try:\n",
    "        cx = x + w/2.\n",
    "        cy = y + h/2.\n",
    "        cx1 = tx * w + cx\n",
    "        cy1 = ty * h + cy\n",
    "        w1 = math.exp(tw) * w\n",
    "        h1 = math.exp(th) * h\n",
    "        x1 = cx1 - w1/2.\n",
    "        y1 = cy1 - h1/2.\n",
    "        x1 = int(round(x1))\n",
    "        y1 = int(round(y1))\n",
    "        w1 = int(round(w1))\n",
    "        h1 = int(round(h1))\n",
    "\n",
    "        return x1, y1, w1, h1\n",
    "\n",
    "    except ValueError:\n",
    "        return x, y, w, h\n",
    "    except OverflowError:\n",
    "        return x, y, w, h\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return x, y, w, h\n",
    "\n",
    "def calc_iou(R, img_data, C, class_mapping):\n",
    "    \"\"\"Converts from (x1,y1,x2,y2) to (x,y,w,h) format\n",
    "\n",
    "    Args:\n",
    "        R: bboxes, probs\n",
    "    \"\"\"\n",
    "    bboxes = img_data['bboxes']\n",
    "    (width, height) = (img_data['width'], img_data['height'])\n",
    "    # get image dimensions for resizing\n",
    "    (resized_width, resized_height) = get_new_img_size(width, height, C.im_size)\n",
    "\n",
    "    gta = np.zeros((len(bboxes), 4))\n",
    "\n",
    "    for bbox_num, bbox in enumerate(bboxes):\n",
    "        # get the GT box coordinates, and resize to account for image resizing\n",
    "        # gta[bbox_num, 0] = (40 * (600 / 800)) / 16 = int(round(1.875)) = 2 (x in feature map)\n",
    "        gta[bbox_num, 0] = int(round(bbox['x1'] * (resized_width / float(width))/C.rpn_stride))\n",
    "        gta[bbox_num, 1] = int(round(bbox['x2'] * (resized_width / float(width))/C.rpn_stride))\n",
    "        gta[bbox_num, 2] = int(round(bbox['y1'] * (resized_height / float(height))/C.rpn_stride))\n",
    "        gta[bbox_num, 3] = int(round(bbox['y2'] * (resized_height / float(height))/C.rpn_stride))\n",
    "\n",
    "    x_roi = []\n",
    "    y_class_num = []\n",
    "    y_class_regr_coords = []\n",
    "    y_class_regr_label = []\n",
    "    IoUs = [] # for debugging only\n",
    "\n",
    "    # R.shape[0]: number of bboxes (=300 from non_max_suppression)\n",
    "    for ix in range(R.shape[0]):\n",
    "        (x1, y1, x2, y2) = R[ix, :]\n",
    "        x1 = int(round(x1))\n",
    "        y1 = int(round(y1))\n",
    "        x2 = int(round(x2))\n",
    "        y2 = int(round(y2))\n",
    "\n",
    "        best_iou = 0.0\n",
    "        best_bbox = -1\n",
    "        # Iterate through all the ground-truth bboxes to calculate the iou\n",
    "        for bbox_num in range(len(bboxes)):\n",
    "            curr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1, y1, x2, y2])\n",
    "\n",
    "            # Find out the corresponding ground-truth bbox_num with larget iou\n",
    "            if curr_iou > best_iou:\n",
    "                best_iou = curr_iou\n",
    "                best_bbox = bbox_num\n",
    "\n",
    "        if best_iou < C.classifier_min_overlap:\n",
    "                continue\n",
    "        else:\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            x_roi.append([x1, y1, w, h])\n",
    "            IoUs.append(best_iou)\n",
    "\n",
    "            if C.classifier_min_overlap <= best_iou < C.classifier_max_overlap:\n",
    "                # hard negative example\n",
    "                cls_name = 'bg'\n",
    "            elif C.classifier_max_overlap <= best_iou:\n",
    "                cls_name = bboxes[best_bbox]['class']\n",
    "                cxg = (gta[best_bbox, 0] + gta[best_bbox, 1]) / 2.0\n",
    "                cyg = (gta[best_bbox, 2] + gta[best_bbox, 3]) / 2.0\n",
    "\n",
    "                cx = x1 + w / 2.0\n",
    "                cy = y1 + h / 2.0\n",
    "\n",
    "                tx = (cxg - cx) / float(w)\n",
    "                ty = (cyg - cy) / float(h)\n",
    "                tw = np.log((gta[best_bbox, 1] - gta[best_bbox, 0]) / float(w))\n",
    "                th = np.log((gta[best_bbox, 3] - gta[best_bbox, 2]) / float(h))\n",
    "            else:\n",
    "                print('roi = {}'.format(best_iou))\n",
    "                raise RuntimeError\n",
    "\n",
    "        class_num = class_mapping[cls_name]\n",
    "        class_label = len(class_mapping) * [0]\n",
    "        class_label[class_num] = 1\n",
    "        y_class_num.append(copy.deepcopy(class_label))\n",
    "        coords = [0] * 4 * (len(class_mapping) - 1)\n",
    "        labels = [0] * 4 * (len(class_mapping) - 1)\n",
    "        if cls_name != 'bg':\n",
    "            label_pos = 4 * class_num\n",
    "            sx, sy, sw, sh = C.classifier_regr_std\n",
    "            coords[label_pos:4+label_pos] = [sx*tx, sy*ty, sw*tw, sh*th]\n",
    "            labels[label_pos:4+label_pos] = [1, 1, 1, 1]\n",
    "            y_class_regr_coords.append(copy.deepcopy(coords))\n",
    "            y_class_regr_label.append(copy.deepcopy(labels))\n",
    "        else:\n",
    "            y_class_regr_coords.append(copy.deepcopy(coords))\n",
    "            y_class_regr_label.append(copy.deepcopy(labels))\n",
    "\n",
    "    if len(x_roi) == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    # bboxes that iou > C.classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes\n",
    "    X = np.array(x_roi)\n",
    "    # one hot code for bboxes from above => x_roi (X)\n",
    "    Y1 = np.array(y_class_num)\n",
    "    # corresponding labels and corresponding gt bboxes\n",
    "    Y2 = np.concatenate([np.array(y_class_regr_label),np.array(y_class_regr_coords)],axis=1)\n",
    "\n",
    "    return np.expand_dims(X, axis=0), np.expand_dims(Y1, axis=0), np.expand_dims(Y2, axis=0), IoUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdBYMThWbU-q"
   },
   "source": [
    "Get new image size and augment the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiCsvijfbSwM"
   },
   "outputs": [],
   "source": [
    "def get_new_img_size(width, height, img_min_side=300):\n",
    "  ''' \n",
    "  Arg       Gets actual width and height and\n",
    "  returns   images sizes under minimum length '''\n",
    "  if width <= height:\n",
    "    f = float(img_min_side) / width\n",
    "    resized_height = int(f * height)\n",
    "    resized_width = img_min_side\n",
    "  else:\n",
    "    f = float(img_min_side) / height\n",
    "    resized_width = int(f * width)\n",
    "    resized_height = img_min_side\n",
    "\n",
    "  return resized_width, resized_height\n",
    "\n",
    "def augment(img_data, config, augment=True):\n",
    "\tassert 'filepath' in img_data\n",
    "\tassert 'bboxes' in img_data\n",
    "\tassert 'width' in img_data\n",
    "\tassert 'height' in img_data\n",
    "\n",
    "\timg_data_aug = copy.deepcopy(img_data)        #copy library\n",
    "\n",
    "\timg = cv2.imread(img_data_aug['filepath'])  # open CV library\n",
    "\n",
    "\tif augment:\n",
    "\t\trows, cols = img.shape[:2]\n",
    "\n",
    "\t\tif config.use_horizontal_flips and np.random.randint(0, 2) == 0:\n",
    "\t\t\timg = cv2.flip(img, 1)\n",
    "\t\t\tfor bbox in img_data_aug['bboxes']:\n",
    "\t\t\t\tx1 = bbox['x1']\n",
    "\t\t\t\tx2 = bbox['x2']\n",
    "\t\t\t\tbbox['x2'] = cols - x1\n",
    "\t\t\t\tbbox['x1'] = cols - x2\n",
    "\n",
    "\t\tif config.use_vertical_flips and np.random.randint(0, 2) == 0:\n",
    "\t\t\timg = cv2.flip(img, 0)\n",
    "\t\t\tfor bbox in img_data_aug['bboxes']:\n",
    "\t\t\t\ty1 = bbox['y1']\n",
    "\t\t\t\ty2 = bbox['y2']\n",
    "\t\t\t\tbbox['y2'] = rows - y1\n",
    "\t\t\t\tbbox['y1'] = rows - y2\n",
    "\n",
    "\t\tif config.rot_90:\n",
    "\t\t\tangle = np.random.choice([0,90,180,270],1)[0]\n",
    "\t\t\tif angle == 270:\n",
    "\t\t\t\timg = np.transpose(img, (1,0,2))\n",
    "\t\t\t\timg = cv2.flip(img, 0)\n",
    "\t\t\telif angle == 180:\n",
    "\t\t\t\timg = cv2.flip(img, -1)\n",
    "\t\t\telif angle == 90:\n",
    "\t\t\t\timg = np.transpose(img, (1,0,2))\n",
    "\t\t\t\timg = cv2.flip(img, 1)\n",
    "\t\t\telif angle == 0:\n",
    "\t\t\t\tpass\n",
    "\n",
    "\t\t\tfor bbox in img_data_aug['bboxes']:\n",
    "\t\t\t\tx1 = bbox['x1']\n",
    "\t\t\t\tx2 = bbox['x2']\n",
    "\t\t\t\ty1 = bbox['y1']\n",
    "\t\t\t\ty2 = bbox['y2']\n",
    "\t\t\t\tif angle == 270:\n",
    "\t\t\t\t\tbbox['x1'] = y1\n",
    "\t\t\t\t\tbbox['x2'] = y2\n",
    "\t\t\t\t\tbbox['y1'] = cols - x2\n",
    "\t\t\t\t\tbbox['y2'] = cols - x1\n",
    "\t\t\t\telif angle == 180:\n",
    "\t\t\t\t\tbbox['x2'] = cols - x1\n",
    "\t\t\t\t\tbbox['x1'] = cols - x2\n",
    "\t\t\t\t\tbbox['y2'] = rows - y1\n",
    "\t\t\t\t\tbbox['y1'] = rows - y2\n",
    "\t\t\t\telif angle == 90:\n",
    "\t\t\t\t\tbbox['x1'] = rows - y2\n",
    "\t\t\t\t\tbbox['x2'] = rows - y1\n",
    "\t\t\t\t\tbbox['y1'] = x1\n",
    "\t\t\t\t\tbbox['y2'] = x2        \n",
    "\t\t\t\telif angle == 0:\n",
    "\t\t\t\t\tpass\n",
    "\n",
    "\timg_data_aug['width'] = img.shape[1]\n",
    "\timg_data_aug['height'] = img.shape[0]\n",
    "\treturn img_data_aug, img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAUBFEM-bhxJ"
   },
   "source": [
    "Generate the ground_truth anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TobshWQabfl4"
   },
   "outputs": [],
   "source": [
    "def get_anchor_gt(all_img_data, C, img_length_calc_function, mode='train'):\n",
    "\t\"\"\" Yield the ground-truth anchors as Y (labels)\n",
    "\t\t\n",
    "\tArgs:\n",
    "\t\tall_img_data here is the list of (filepath, width, height, list(bboxes))\n",
    "\t\tC: configuration function coded above\n",
    "\t\timg_length_calc_function is the function to calculate final layer's feature map (of base model) size according to input image size\n",
    "\t\tmode: 'train' or 'test'; 'train' mode need augmentation\n",
    "\n",
    "\tReturns:\n",
    "\t\tx_img: image data after resized and scaling (smallest size of leaf is 256px)\n",
    "\t\tY: [y_rpn_cls, y_rpn_regr]\n",
    "\t\timg_data_aug: augmented image data (original image with augmentation)\n",
    "\t\tdebug_img: show image for debug\n",
    "\t\tnum_pos: show number of positive anchors for debug\n",
    "\t\"\"\"\n",
    "\twhile True:\n",
    "\n",
    "\t\tfor img_data in all_img_data:\n",
    "\t\t\ttry:\n",
    "\n",
    "\t\t\t\t# read in image, and optionally add augmentation\n",
    "\n",
    "\t\t\t\tif mode == 'train':\n",
    "\t\t\t\t\timg_data_aug, x_img = augment(img_data, C, augment=True)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\timg_data_aug, x_img = augment(img_data, C, augment=False)\n",
    "\n",
    "\t\t\t\t(width, height) = (img_data_aug['width'], img_data_aug['height'])\n",
    "\t\t\t\t(rows, cols, _) = x_img.shape\n",
    "\n",
    "\t\t\t\tassert cols == width\n",
    "\t\t\t\tassert rows == height\n",
    "\n",
    "\t\t\t\t# get image dimensions for resizing\n",
    "\t\t\t\t(resized_width, resized_height) = get_new_img_size(width, height, C.im_size)\n",
    "\n",
    "\t\t\t\t# resize the image so that smalles side is length = 300px\n",
    "\t\t\t\tx_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)\n",
    "\t\t\t\tdebug_img = x_img.copy()\n",
    "\n",
    "\n",
    "        # it needs calc_rpn function\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\ty_rpn_cls, y_rpn_regr, num_pos = calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t# Zero-center by mean pixel, and preprocess image\n",
    "\n",
    "\t\t\t\tx_img = x_img[:,:, (2, 1, 0)]  # BGR -> RGB\n",
    "\t\t\t\tx_img = x_img.astype(np.float32)\n",
    "\t\t\t\tx_img[:, :, 0] -= C.img_channel_mean[0]\n",
    "\t\t\t\tx_img[:, :, 1] -= C.img_channel_mean[1]\n",
    "\t\t\t\tx_img[:, :, 2] -= C.img_channel_mean[2]\n",
    "\t\t\t\tx_img /= C.img_scaling_factor\n",
    "\n",
    "\t\t\t\tx_img = np.transpose(x_img, (2, 0, 1))\n",
    "\t\t\t\tx_img = np.expand_dims(x_img, axis=0)\n",
    "\n",
    "\t\t\t\ty_rpn_regr[:, y_rpn_regr.shape[1]//2:, :, :] *= C.std_scaling  ### std_scaling calculated in C config\n",
    "\n",
    "\t\t\t\tx_img = np.transpose(x_img, (0, 2, 3, 1))\n",
    "\t\t\t\ty_rpn_cls = np.transpose(y_rpn_cls, (0, 2, 3, 1))\n",
    "\t\t\t\ty_rpn_regr = np.transpose(y_rpn_regr, (0, 2, 3, 1))\n",
    "\n",
    "\t\t\t\tyield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug, debug_img, num_pos\n",
    "\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(e)\n",
    "\t\t\t\tcontinue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cq_V3aCjcgy8"
   },
   "source": [
    "#Set of function for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTpyKv4scboi"
   },
   "outputs": [],
   "source": [
    "lambda_rpn_regr = 1.0\n",
    "lambda_rpn_class = 1.0\n",
    "\n",
    "lambda_cls_regr = 1.0\n",
    "lambda_cls_class = 1.0\n",
    "\n",
    "epsilon = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99FwSSCqclZ0"
   },
   "outputs": [],
   "source": [
    "def rpn_loss_regr(num_anchors):\n",
    "    \"\"\"Loss function for rpn regression\n",
    "    Args:\n",
    "        num_anchors: number of anchors (9 in here)\n",
    "    Returns:\n",
    "        Smooth L1 loss function \n",
    "                           0.5*x*x (if x_abs < 1)\n",
    "                           x_abx - 0.5 (otherwise)\n",
    "    \"\"\"\n",
    "    def rpn_loss_regr_fixed_num(y_true, y_pred):\n",
    "\n",
    "        # x is the difference between true value and predicted vaue\n",
    "        x = y_true[:, :, :, 4 * num_anchors:] - y_pred\n",
    "\n",
    "        # absolute value of x\n",
    "        x_abs = K.abs(x)\n",
    "\n",
    "        # If x_abs <= 1.0, x_bool = 1\n",
    "        x_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)\n",
    "\n",
    "        return lambda_rpn_regr * K.sum(\n",
    "            y_true[:, :, :, :4 * num_anchors] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :, :4 * num_anchors])\n",
    "\n",
    "    return rpn_loss_regr_fixed_num\n",
    "\n",
    "\n",
    "def rpn_loss_cls(num_anchors):\n",
    "    \"\"\"Loss function for rpn classification\n",
    "    Args:\n",
    "        num_anchors: number of anchors (9 in here)\n",
    "        y_true[:, :, :, :9]: [0,1,0,0,0,0,0,1,0] means only the second and the eighth box is valid which contains pos or neg anchor => isValid\n",
    "        y_true[:, :, :, 9:]: [0,1,0,0,0,0,0,0,0] means the second box is pos and eighth box is negative\n",
    "    Returns:\n",
    "        lambda * sum((binary_crossentropy(isValid*y_pred,y_true))) / N\n",
    "    \"\"\"\n",
    "    def rpn_loss_cls_fixed_num(y_true, y_pred):\n",
    "\n",
    "            return lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])\n",
    "\n",
    "    return rpn_loss_cls_fixed_num\n",
    "\n",
    "\n",
    "def class_loss_regr(num_classes):\n",
    "    \"\"\"Loss function for rpn regression\n",
    "    Args:\n",
    "        num_anchors: number of anchors (9 in here)\n",
    "    Returns:\n",
    "        Smooth L1 loss function \n",
    "                           0.5*x*x (if x_abs < 1)\n",
    "                           x_abx - 0.5 (otherwise)\n",
    "    \"\"\"\n",
    "    def class_loss_regr_fixed_num(y_true, y_pred):\n",
    "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
    "        x_abs = K.abs(x)\n",
    "        x_bool = K.cast(K.less_equal(x_abs, 1.0), 'float32')\n",
    "        return lambda_cls_regr * K.sum(y_true[:, :, :4*num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :4*num_classes])\n",
    "    return class_loss_regr_fixed_num\n",
    "\n",
    "\n",
    "def class_loss_cls(y_true, y_pred):\n",
    "    return lambda_cls_class * K.mean(categorical_crossentropy(y_true[0, :, :], y_pred[0, :, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVlOKjaDcrpC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df5k76UkAeQ5"
   },
   "source": [
    "#Final layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fVTAByIar6N"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "ROI Pooling Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4RqiPksarCv"
   },
   "outputs": [],
   "source": [
    "class RoiPoolingConv(Layer):\n",
    "    '''\n",
    "    ROI pooling layer for 2D inputs.\n",
    "    # Arguments\n",
    "        pool_size: int\n",
    "            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.\n",
    "            num_rois: number of regions of interest to be used\n",
    "    # It takes in two tensors:                  $$$ See the function call $$$\n",
    "      1) A batch of images.                     In order to be able to process them together, all images must have the same shape. \n",
    "                                                The resulting shape of the tensor will be (batch_size, img_width, img_height, n_channels).\n",
    "      2) A batch of (ROI) proposals.            If we want to stack them together in a tensor, the number of proposed regions must be fixed for each image. \n",
    "                                                Since each bounding box must be specified with 4 coordinates, \n",
    "                                                the shape of this tensor will be (batch_size, n_rois, 4)\n",
    "    # Input shape\n",
    "        list of two 4D tensors [X_img,X_roi] with shape:\n",
    "        X_img:\n",
    "        `(1, rows, cols, channels)`\n",
    "        X_roi:\n",
    "        `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n",
    "    # It outputs a 3D tensor:\n",
    "      A list of embeddings for each image, codifying the regions specified by each ROI. \n",
    "      \n",
    "    # Output shape\n",
    "        (1, num_rois, channels, pool_size, pool_size)` $$$see the function compute_output_shape$$$\n",
    "        (batch_size, n_rois, pooled_width, pooled_height, n_channels).\n",
    "    '''\n",
    "    def __init__(self, pool_size, num_rois, **kwargs):\n",
    "\n",
    "        self.dim_ordering = K.image_data_format()\n",
    "        self.pool_size = pool_size\n",
    "        self.num_rois = num_rois\n",
    "\n",
    "        super(RoiPoolingConv, self).__init__(**kwargs) # we call the parent constructor to initialize the rest of the class attributes\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.nb_channels = input_shape[0][3]   \n",
    "\n",
    "    def compute_output_shape(self, input_shape):      #  It'll tell us what the output of the layer will be for a particular input, we used it in next to next cell\n",
    "        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n",
    "\n",
    "    def call(self, x, mask=None):       # This is the main function representing the ROIPooling\n",
    "\n",
    "        assert(len(x) == 2)\n",
    "\n",
    "        # x[0] is image with shape (rows, cols, channels)\n",
    "        img = x[0] \n",
    "\n",
    "        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)\n",
    "        rois = x[1]\n",
    "\n",
    "        input_shape = K.shape(img)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for roi_idx in range(self.num_rois): # segregating x,y,w,h values for each ROI input\n",
    "\n",
    "            x = rois[0, roi_idx, 0]\n",
    "            y = rois[0, roi_idx, 1]\n",
    "            w = rois[0, roi_idx, 2]\n",
    "            h = rois[0, roi_idx, 3]\n",
    "\n",
    "            x = K.cast(x, 'int32')\n",
    "            y = K.cast(y, 'int32')\n",
    "            w = K.cast(w, 'int32')\n",
    "            h = K.cast(h, 'int32')\n",
    "\n",
    "            # Resized roi of the image to pooling size (7x7)\n",
    "            rs = tf.image.resize(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))   #compressing via pooling method\n",
    "            outputs.append(rs)\n",
    "                \n",
    "\n",
    "        final_output = K.concatenate(outputs, axis=0)\n",
    "\n",
    "        # Reshape to (1, num_rois, pool_size, pool_size, nb_channels)\n",
    "        # Might be (1, 4, 7, 7, 3)\n",
    "        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n",
    "\n",
    "        # permute_dimensions is similar to transpose\n",
    "        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n",
    "\n",
    "        return final_output\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'pool_size': self.pool_size,\n",
    "                  'num_rois': self.num_rois}\n",
    "        base_config = super(RoiPoolingConv, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s6slLcybCqZ"
   },
   "source": [
    "Classifier Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_T2M44a1bB4z"
   },
   "outputs": [],
   "source": [
    "def classifier_layer(base_layers, input_rois, num_rois, nb_classes = 4):\n",
    "    \"\"\"Create a classifier layer\n",
    "    \n",
    "    Args:\n",
    "        base_layers: vgg\n",
    "        input_rois: `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n",
    "        num_rois: number of rois to be processed in one time (4 in here)\n",
    "\n",
    "    Returns:\n",
    "        list(out_class, out_regr)\n",
    "        out_class: classifier layer output\n",
    "        out_regr: regression layer output\n",
    "    \"\"\"\n",
    "\n",
    "    input_shape = (num_rois,7,7,512)\n",
    "\n",
    "    pooling_regions = 7\n",
    "\n",
    "    # out_roi_pool.shape = (1, num_rois, channels, pool_size, pool_size)\n",
    "    # num_rois (4) 7x7 roi pooling\n",
    "    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n",
    "\n",
    "    # Flatten the convlutional layer and connected to 2 FC and 2 dropout\n",
    "    out = TimeDistributed(Flatten(name='flatten'))(out_roi_pool)\n",
    "    out = TimeDistributed(Dense(4096, activation='relu', name='fc1'))(out)\n",
    "    out = TimeDistributed(Dropout(0.5))(out)\n",
    "    out = TimeDistributed(Dense(4096, activation='relu', name='fc2'))(out)\n",
    "    out = TimeDistributed(Dropout(0.5))(out)\n",
    "\n",
    "    # These are the two output layer\n",
    "    # out_class: softmax acivation function for classify the class name of the object\n",
    "    # out_regr: linear activation function for bboxes coordinates regression\n",
    "    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n",
    "    # note: no regression target for bg class\n",
    "    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n",
    "\n",
    "    return [out_class, out_regr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7ZKWCe82sON"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VTZdK2hi5f_"
   },
   "source": [
    "#Calculate the rpn for all anchors of all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3cczNnWbKps"
   },
   "outputs": [],
   "source": [
    "def calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):\n",
    "\t\"\"\" \n",
    "\tArguments:\n",
    "\t\tC: config we did on #1 Preprocessing part\n",
    "\t\timg_data: augmented image data\n",
    "\t\twidth: original image width                                   (e.g. 600)\n",
    "\t\theight: original image height                                 (e.g. 800)\n",
    "\t\tresized_width: resized image width according to C.im_size     (e.g. 300)\n",
    "\t\tresized_height: resized image height according to C.im_size   (e.g. 400)\n",
    "\t\timg_length_calc_function: #compute_output_shape function of RoiPoolingConv class above\n",
    "    function to calculate final layer's feature map (of base model) size according to input image size\n",
    "\n",
    "\tReturns:\n",
    "\t\ty_rpn_cls: list(num_bboxes, y_is_box_valid + y_rpn_overlap)\n",
    "\t\t\ty_is_box_valid: 0 or 1 (0 means the box is invalid, 1 means the box is valid)\n",
    "\t\t\ty_rpn_overlap: 0 or 1 (0 means the box is not an object, 1 means the box is an object)\n",
    "\t\ty_rpn_regr: list(num_bboxes, 4*y_rpn_overlap + y_rpn_regr)\n",
    "\t\t\ty_rpn_regr: x1,y1,x2,y2 bounding boxes coordinates\n",
    "\t\"\"\"\n",
    "\tdownscale = float(C.rpn_stride)                      # 16\n",
    "\tanchor_sizes = C.anchor_box_scales                   # 128, 256, 512\n",
    "\tanchor_ratios = C.anchor_box_ratios                  # 1:1, 1:2, 2:1\n",
    "\tnum_anchors = len(anchor_sizes) * len(anchor_ratios) # 3x3=9\n",
    "\n",
    "\t# calculate the output map size based on the network architecture\n",
    "\t(output_width, output_height) = img_length_calc_function(resized_width, resized_height)\n",
    "\n",
    "\tn_anchratios = len(anchor_ratios)                   # 3\n",
    "\t\n",
    "\t# initialise empty output objectives\n",
    "\ty_rpn_overlap = np.zeros((output_height, output_width, num_anchors))\n",
    "\ty_is_box_valid = np.zeros((output_height, output_width, num_anchors))\n",
    "\ty_rpn_regr = np.zeros((output_height, output_width, num_anchors * 4))\n",
    "\n",
    "\tnum_bboxes = len(img_data['bboxes'])\n",
    "\n",
    "\tnum_anchors_for_bbox = np.zeros(num_bboxes).astype(int)\n",
    "\tbest_anchor_for_bbox = -1*np.ones((num_bboxes, 4)).astype(int)\n",
    "\tbest_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32)\n",
    "\tbest_x_for_bbox = np.zeros((num_bboxes, 4)).astype(int)\n",
    "\tbest_dx_for_bbox = np.zeros((num_bboxes, 4)).astype(np.float32)\n",
    "\n",
    "\t# get the GT box coordinates, and resize to account for image resizing\n",
    "\tgta = np.zeros((num_bboxes, 4))\n",
    "\tfor bbox_num, bbox in enumerate(img_data['bboxes']):\n",
    "\t\t# get the GT box coordinates, and resize to account for image resizing\n",
    "\t\tgta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))\n",
    "\t\tgta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))\n",
    "\t\tgta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))\n",
    "\t\tgta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))\n",
    "\t\n",
    "\t# rpn ground truth\n",
    "\n",
    "\tfor anchor_size_idx in range(len(anchor_sizes)):\n",
    "\t\tfor anchor_ratio_idx in range(n_anchratios):\n",
    "\t\t\tanchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]\n",
    "\t\t\tanchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]\t\n",
    "\t\t\tfor ix in range(output_width):\t\t\t\t\t\n",
    "\t\t\t\t# x-coordinates of the current anchor box\t\n",
    "\t\t\t\tx1_anc = downscale * (ix + 0.5) - anchor_x / 2\n",
    "\t\t\t\tx2_anc = downscale * (ix + 0.5) + anchor_x / 2\t\n",
    "\t\t\t\t\n",
    "\t\t\t\t# ignore boxes that go across image boundaries\t\t\t\t\t\n",
    "\t\t\t\tif x1_anc < 0 or x2_anc > resized_width:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t\t\n",
    "\t\t\t\tfor jy in range(output_height):\n",
    "\n",
    "\t\t\t\t\t# y-coordinates of the current anchor box\n",
    "\t\t\t\t\ty1_anc = downscale * (jy + 0.5) - anchor_y / 2\n",
    "\t\t\t\t\ty2_anc = downscale * (jy + 0.5) + anchor_y / 2\n",
    "\n",
    "\t\t\t\t\t# ignore boxes that go across image boundaries\n",
    "\t\t\t\t\tif y1_anc < 0 or y2_anc > resized_height:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\t# bbox_type indicates whether an anchor should be a target\n",
    "\t\t\t\t\t# Initialize with 'negative'\n",
    "\t\t\t\t\tbbox_type = 'neg'\n",
    "\n",
    "\t\t\t\t\t# this is the best IOU for the (x,y) coord and the current anchor\n",
    "\t\t\t\t\t# note that this is different from the best IOU for a GT bbox\n",
    "\t\t\t\t\tbest_iou_for_loc = 0.0\n",
    "\n",
    "\t\t\t\t\tfor bbox_num in range(num_bboxes):\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# get IOU of the current GT box and the current anchor box\n",
    "\t\t\t\t\t\tcurr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1_anc, y1_anc, x2_anc, y2_anc])\n",
    "\t\t\t\t\t\t# calculate the regression targets if they will be needed\n",
    "\t\t\t\t\t\tif curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > C.rpn_max_overlap:\n",
    "\t\t\t\t\t\t\tcx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0\n",
    "\t\t\t\t\t\t\tcy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0\n",
    "\t\t\t\t\t\t\tcxa = (x1_anc + x2_anc)/2.0\n",
    "\t\t\t\t\t\t\tcya = (y1_anc + y2_anc)/2.0\n",
    "\n",
    "\t\t\t\t\t\t\t# x,y are the center point of ground-truth bbox\n",
    "\t\t\t\t\t\t\t# xa,ya are the center point of anchor bbox (xa=downscale * (ix + 0.5); ya=downscale * (iy+0.5))\n",
    "\t\t\t\t\t\t\t# w,h are the width and height of ground-truth bbox\n",
    "\t\t\t\t\t\t\t# wa,ha are the width and height of anchor bboxe\n",
    "\t\t\t\t\t\t\t# tx = (x - xa) / wa\n",
    "\t\t\t\t\t\t\t# ty = (y - ya) / ha\n",
    "\t\t\t\t\t\t\t# tw = log(w / wa)\n",
    "\t\t\t\t\t\t\t# th = log(h / ha)\n",
    "\t\t\t\t\t\t\ttx = (cx - cxa) / (x2_anc - x1_anc)\n",
    "\t\t\t\t\t\t\tty = (cy - cya) / (y2_anc - y1_anc)\n",
    "\t\t\t\t\t\t\ttw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))\n",
    "\t\t\t\t\t\t\tth = np.log((gta[bbox_num, 3] - gta[bbox_num, 2]) / (y2_anc - y1_anc))\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tif img_data['bboxes'][bbox_num]['class'] != 'bg':\n",
    "\n",
    "\t\t\t\t\t\t\t# all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best\n",
    "\t\t\t\t\t\t\tif curr_iou > best_iou_for_bbox[bbox_num]:\n",
    "\t\t\t\t\t\t\t\tbest_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]\n",
    "\t\t\t\t\t\t\t\tbest_iou_for_bbox[bbox_num] = curr_iou\n",
    "\t\t\t\t\t\t\t\tbest_x_for_bbox[bbox_num,:] = [x1_anc, x2_anc, y1_anc, y2_anc]\n",
    "\t\t\t\t\t\t\t\tbest_dx_for_bbox[bbox_num,:] = [tx, ty, tw, th]\n",
    "\n",
    "\t\t\t\t\t\t\t# we set the anchor to positive if the IOU is >0.7 (it does not matter if there was another better box, it just indicates overlap)\n",
    "\t\t\t\t\t\t\tif curr_iou > C.rpn_max_overlap:\n",
    "\t\t\t\t\t\t\t\tbbox_type = 'pos'\n",
    "\t\t\t\t\t\t\t\tnum_anchors_for_bbox[bbox_num] += 1\n",
    "\t\t\t\t\t\t\t\t# we update the regression layer target if this IOU is the best for the current (x,y) and anchor position\n",
    "\t\t\t\t\t\t\t\tif curr_iou > best_iou_for_loc:\n",
    "\t\t\t\t\t\t\t\t\tbest_iou_for_loc = curr_iou\n",
    "\t\t\t\t\t\t\t\t\tbest_regr = (tx, ty, tw, th)\n",
    "\n",
    "\t\t\t\t\t\t\t# if the IOU is >0.3 and <0.7, it is ambiguous and no included in the objective\n",
    "\t\t\t\t\t\t\tif C.rpn_min_overlap < curr_iou < C.rpn_max_overlap:\n",
    "\t\t\t\t\t\t\t\t# gray zone between neg and pos\n",
    "\t\t\t\t\t\t\t\tif bbox_type != 'pos':\n",
    "\t\t\t\t\t\t\t\t\tbbox_type = 'neutral'\n",
    "\n",
    "\t\t\t\t\t# turn on or off outputs depending on IOUs\n",
    "\t\t\t\t\tif bbox_type == 'neg':\n",
    "\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n",
    "\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n",
    "\t\t\t\t\telif bbox_type == 'neutral':\n",
    "\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n",
    "\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n",
    "\t\t\t\t\telif bbox_type == 'pos':\n",
    "\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n",
    "\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n",
    "\t\t\t\t\t\tstart = 4 * (anchor_ratio_idx + n_anchratios * anchor_size_idx)\n",
    "\t\t\t\t\t\ty_rpn_regr[jy, ix, start:start+4] = best_regr\n",
    "\n",
    "\t# we ensure that every bbox has at least one positive RPN region\n",
    "\n",
    "\tfor idx in range(num_anchors_for_bbox.shape[0]):\n",
    "\t\tif num_anchors_for_bbox[idx] == 0:\n",
    "\t\t\t# no box with an IOU greater than zero ...\n",
    "\t\t\tif best_anchor_for_bbox[idx, 0] == -1:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\ty_is_box_valid[\n",
    "\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n",
    "\t\t\t\tbest_anchor_for_bbox[idx,3]] = 1\n",
    "\t\t\ty_rpn_overlap[\n",
    "\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n",
    "\t\t\t\tbest_anchor_for_bbox[idx,3]] = 1\n",
    "\t\t\tstart = 4 * (best_anchor_for_bbox[idx,2] + n_anchratios * best_anchor_for_bbox[idx,3])\n",
    "\t\t\ty_rpn_regr[\n",
    "\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], start:start+4] = best_dx_for_bbox[idx, :]\n",
    "\n",
    "\ty_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n",
    "\ty_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n",
    "\n",
    "\ty_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n",
    "\ty_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n",
    "\n",
    "\ty_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))\n",
    "\ty_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n",
    "\n",
    "\tpos_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 1, y_is_box_valid[0, :, :, :] == 1))\n",
    "\tneg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))\n",
    "\n",
    "\tnum_pos = len(pos_locs[0])\n",
    "\n",
    "\t# one issue is that the RPN has many more negative than positive regions, so we turn off some of the negative\n",
    "\t# regions. We also limit it to 256 regions.\n",
    "\tnum_regions = 256\n",
    "\n",
    "\tif len(pos_locs[0]) > num_regions/2:\n",
    "\t\tval_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions/2)\n",
    "\t\ty_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n",
    "\t\tnum_pos = num_regions/2\n",
    "\n",
    "\tif len(neg_locs[0]) + num_pos > num_regions:\n",
    "\t\tval_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)\n",
    "\t\ty_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n",
    "\n",
    "\ty_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=1)\n",
    "\ty_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=1), y_rpn_regr], axis=1)\n",
    "\n",
    "\treturn np.copy(y_rpn_cls), np.copy(y_rpn_regr), num_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHPrfthSboYX"
   },
   "outputs": [],
   "source": [
    "def rpn_to_roi(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=300,overlap_thresh=0.9):\n",
    "\t\"\"\"Convert rpn layer to roi bboxes\n",
    "\n",
    "\tArgs: (num_anchors = 9)\n",
    "\t\trpn_layer: output layer for rpn classification \n",
    "\t\t\tshape (1, feature_map.height, feature_map.width, num_anchors)\n",
    "\t\t\tMight be (1, 18, 25, 9) if resized image is 400 width and 300\n",
    "\t\tregr_layer: output layer for rpn regression\n",
    "\t\t\tshape (1, feature_map.height, feature_map.width, num_anchors)\n",
    "\t\t\tMight be (1, 18, 25, 36) if resized image is 400 width and 300\n",
    "\t\tC: config\n",
    "\t\tuse_regr: Whether to use bboxes regression in rpn\n",
    "\t\tmax_boxes: max bboxes number for non-max-suppression (NMS)\n",
    "\t\toverlap_thresh: If iou in NMS is larger than this threshold, drop the box\n",
    "\n",
    "\tReturns:\n",
    "\t\tresult: boxes from non-max-suppression (shape=(300, 4))\n",
    "\t\t\tboxes: coordinates for bboxes (on the feature map)\n",
    "\t\"\"\"\n",
    "\tregr_layer = regr_layer / C.std_scaling\n",
    "\n",
    "\tanchor_sizes = C.anchor_box_scales   # (3 in here)\n",
    "\tanchor_ratios = C.anchor_box_ratios  # (3 in here)\n",
    "\n",
    "\tassert rpn_layer.shape[0] == 1\n",
    "\n",
    "\t(rows, cols) = rpn_layer.shape[1:3]\n",
    "\n",
    "\tcurr_layer = 0\n",
    "\n",
    "\t# A.shape = (4, feature_map.height, feature_map.width, num_anchors) \n",
    "\t# Might be (4, 18, 25, 9) if resized image is 400 width and 300\n",
    "\t# A is the coordinates for 9 anchors for every point in the feature map \n",
    "\t# => all 18x25x9=4050 anchors cooridnates\n",
    "\tA = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))\n",
    "\n",
    "\tfor anchor_size in anchor_sizes:\n",
    "\t\tfor anchor_ratio in anchor_ratios:\n",
    "\t\t\t# anchor_x = (128 * 1) / 16 = 8  => width of current anchor\n",
    "\t\t\t# anchor_y = (128 * 2) / 16 = 16 => height of current anchor\n",
    "\t\t\tanchor_x = (anchor_size * anchor_ratio[0])/C.rpn_stride\n",
    "\t\t\tanchor_y = (anchor_size * anchor_ratio[1])/C.rpn_stride\n",
    "\t\t\t\n",
    "\t\t\t# curr_layer: 0~8 (9 anchors)\n",
    "\t\t\t# the Kth anchor of all position in the feature map (9th in total)\n",
    "\t\t\tregr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4] # shape => (18, 25, 4)\n",
    "\t\t\tregr = np.transpose(regr, (2, 0, 1)) # shape => (4, 18, 25)\n",
    "\n",
    "\t\t\t# Create 18x25 mesh grid\n",
    "\t\t\t# For every point in x, there are all the y points and vice versa\n",
    "\t\t\t# X.shape = (18, 25)\n",
    "\t\t\t# Y.shape = (18, 25)\n",
    "\t\t\tX, Y = np.meshgrid(np.arange(cols),np. arange(rows))\n",
    "\n",
    "\t\t\t# Calculate anchor position and size for each feature map point\n",
    "\t\t\tA[0, :, :, curr_layer] = X - anchor_x/2 # Top left x coordinate\n",
    "\t\t\tA[1, :, :, curr_layer] = Y - anchor_y/2 # Top left y coordinate\n",
    "\t\t\tA[2, :, :, curr_layer] = anchor_x       # width of current anchor\n",
    "\t\t\tA[3, :, :, curr_layer] = anchor_y       # height of current anchor\n",
    "\n",
    "\t\t\t# Apply regression to x, y, w and h if there is rpn regression layer\n",
    "\t\t\tif use_regr:\n",
    "\t\t\t\tA[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)\n",
    "\n",
    "\t\t\t# Avoid width and height exceeding 1\n",
    "\t\t\tA[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])\n",
    "\t\t\tA[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])\n",
    "\n",
    "\t\t\t# Convert (x, y , w, h) to (x1, y1, x2, y2)\n",
    "\t\t\t# x1, y1 is top left coordinate\n",
    "\t\t\t# x2, y2 is bottom right coordinate\n",
    "\t\t\tA[2, :, :, curr_layer] += A[0, :, :, curr_layer]\n",
    "\t\t\tA[3, :, :, curr_layer] += A[1, :, :, curr_layer]\n",
    "\n",
    "\t\t\t# Avoid bboxes drawn outside the feature map\n",
    "\t\t\tA[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])\n",
    "\t\t\tA[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])\n",
    "\t\t\tA[2, :, :, curr_layer] = np.minimum(cols-1, A[2, :, :, curr_layer])\n",
    "\t\t\tA[3, :, :, curr_layer] = np.minimum(rows-1, A[3, :, :, curr_layer])\n",
    "\n",
    "\t\t\tcurr_layer += 1\n",
    "\n",
    "\tall_boxes = np.reshape(A.transpose((0, 3, 1, 2)), (4, -1)).transpose((1, 0))  # shape=(4050, 4)\n",
    "\tall_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))                   # shape=(4050,) flattening\n",
    "\n",
    "\tx1 = all_boxes[:, 0]\n",
    "\ty1 = all_boxes[:, 1]\n",
    "\tx2 = all_boxes[:, 2]\n",
    "\ty2 = all_boxes[:, 3]\n",
    "\n",
    "\t# Find out the bboxes which is illegal and delete them from bboxes list\n",
    "\tidxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n",
    "\n",
    "\tall_boxes = np.delete(all_boxes, idxs, 0)\n",
    "\tall_probs = np.delete(all_probs, idxs, 0)\n",
    "\n",
    "\t# Apply non_max_suppression\n",
    "\t# Only extract the bboxes. Don't need rpn probs in the later process\n",
    "\tresult = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]\n",
    "\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4UCKfbbpzGY"
   },
   "source": [
    "#loading important files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LNFcdHhbrsg"
   },
   "outputs": [],
   "source": [
    "base_path = 'drive/MyDrive/FRCNN/keras-frcnn-master/keras_frcnn/new'\n",
    "train_path = 'drive/MyDrive/FRCNN/keras-frcnn-master/train.txt'\n",
    "num_rois = 4 # Number of RoIs to process at once.\n",
    "# Augmentation flag\n",
    "horizontal_flips = True # Augment with horizontal flips in training. \n",
    "vertical_flips = True   # Augment with vertical flips in training. \n",
    "rot_90 = True           # Augment with 90 degree rotations in training. \n",
    "output_weight_path = os.path.join(base_path, 'the_model_frcnn_vgg.hdf5')\n",
    "record_path = os.path.join(base_path, 'record.csv') # Record data (used to save the losses, classification accuracy and mean average precision)\n",
    "base_weight_path = 'drive/MyDrive/FRCNN/keras-frcnn-master/keras_frcnn/new/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "config_output_filename = os.path.join(base_path, 'model_vgg_config.pickle')\n",
    "test_base_path = 'drive/MyDrive/FRCNN/test'\n",
    "test_path = '/content/drive/MyDrive/FRCNN/keras-frcnn/annottest.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVBu_hwUkKx1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgH55VPe8i55"
   },
   "source": [
    "#Sample out 10% & Testing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mk5mn1LRjEWI"
   },
   "outputs": [],
   "source": [
    "allimages = pd.read_csv(base_path+'/PlantVillageDoc.txt',names=  ['parsedxml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aAqT0RkOodSw",
    "outputId": "3d47e7ba-9c41-4eea-82f7-382d81b2560d"
   },
   "outputs": [],
   "source": [
    "len(allimages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sKdfyjHGodXE",
    "outputId": "5eea347a-f67d-4660-91e0-379897bc16b9"
   },
   "outputs": [],
   "source": [
    "allimages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pWz4KuRbXve",
    "outputId": "22b62d08-986e-47e7-f578-b5d9f62ad2d5"
   },
   "outputs": [],
   "source": [
    "print(allimages['parsedxml'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43diSKa0n2SJ"
   },
   "outputs": [],
   "source": [
    "#Villages and docs leaves are separated\n",
    "plantdoc =[]\n",
    "plantdocs =[]\n",
    "plantvillage=[]\n",
    "plantvillages=[]\n",
    "stratify =[]\n",
    "\n",
    "for i in allimages['parsedxml']:\n",
    "  if \"PlantVillage\" in i.split(',')[0]:\n",
    "    plantvillage.append(i.split(',')[0])\n",
    "    plantvillages.append(i)\n",
    "    stratify.append(i.split(',')[-1])\n",
    "  else:\n",
    "    plantdoc.append(i.split(',')[0])\n",
    "    plantdocs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tut5I7eXKR7i",
    "outputId": "48d0c044-efc5-4f3e-c215-17408a7ca6df"
   },
   "outputs": [],
   "source": [
    "print(len(plantdocs),len(plantvillage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hb9DwJJPKbwQ"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(base_path+'/CompleteImages.csv')\n",
    "df= df.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bikD6dH6f2Ne",
    "outputId": "39f09e08-2937-4b19-d639-d86489cbe10b"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uj5DLXNTGx-s"
   },
   "outputs": [],
   "source": [
    "finaldf =pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWt8NVoRF-gF"
   },
   "outputs": [],
   "source": [
    "finaldf[\"one\"] = df[\"filepath\"].astype(str) + df[\"width\"].astype(str)+ df[\"height\"].astype(str)+ df[\"bboxes\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kA5qJymVHD26"
   },
   "outputs": [],
   "source": [
    "finaldf.to_csv(base_path+'/finaldf.txt',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yrSY_5JXNchd",
    "outputId": "fa2ea66d-19f9-4025-f057-f3102016a892"
   },
   "outputs": [],
   "source": [
    "#plantdoc are segregated with class\n",
    "docsaddress = []\n",
    "for i in df['index']:\n",
    "  try:\n",
    "    if \"PlantDoc\" in df['filepath'].iloc[i]:\n",
    "      docsaddress.append(df.iloc[i])\n",
    "  except:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lf5TL7JPqiq1"
   },
   "outputs": [],
   "source": [
    "dfff = pd.DataFrame(docsaddress)\n",
    "dfff.to_csv('docss.txt',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9U3zV_06riFd",
    "outputId": "08618c81-108d-4b09-e766-014efbb29163"
   },
   "outputs": [],
   "source": [
    "dfff['filepath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekhR5CRUluLM",
    "outputId": "b18910e5-4d28-4151-a714-816bed11131d"
   },
   "outputs": [],
   "source": [
    "dfdocss = pd.read_csv('docss.txt')\n",
    "dfdocss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhCI2dxGlmMs",
    "outputId": "470c6b1a-641e-42d9-afe1-9d82ad9333d2"
   },
   "outputs": [],
   "source": [
    "df.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKZauL4nOhnW",
    "outputId": "a40f95a0-beb2-48d9-b4a7-3997de1c094b"
   },
   "outputs": [],
   "source": [
    "len(docsaddress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4mLfWPkPUsu"
   },
   "outputs": [],
   "source": [
    "docsaddress = list(dfff['filepath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XlEPnTElO8s-",
    "outputId": "a17f9873-7a03-40eb-e244-afe5bb45b055"
   },
   "outputs": [],
   "source": [
    "# Splitting 10 then appending to PlantDocs\n",
    "#Picking 10% of plant village images to \n",
    "from sklearn.model_selection import train_test_split\n",
    "train_doc_images,test_doc_images,train_doc_class,train_doc_class = train_test_split(docsaddress,docsaddress,test_size=0.041)\n",
    "print(\"train_doc_images:\",len(train_doc_images),\"test_doc_images:\",len(test_doc_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVJ2x1BkRx64",
    "outputId": "55472f5f-861a-4a5b-fd07-c16a46e667d5"
   },
   "outputs": [],
   "source": [
    "#THIS IS FOR LET TRAINING HAPPEN\n",
    "docsdict = []\n",
    "testdocs = []\n",
    "for i in plantdocs:\n",
    "  loc = str(i.split(',')[0])\n",
    "  if loc in train_doc_images:\n",
    "    docsdict.append(i)\n",
    "  else:\n",
    "    testdocs.append(i)\n",
    "print(len(docsdict),len(testdocs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPprPjBCSaVy",
    "outputId": "27cefbdd-0606-481f-edbc-487b4370fe72"
   },
   "outputs": [],
   "source": [
    "print(\"Under\",len(train_doc_images),\"images of Plant docs \" , len(docsdict), \"leaves are collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tqob9OX0XQuz",
    "outputId": "4c34286b-94d6-4494-f07d-73b469fe3bb0"
   },
   "outputs": [],
   "source": [
    "# Splitting 10 then appending to PlantDocs\n",
    "#Picking 10% of plant village images to balance out plant doc images \n",
    "from sklearn.model_selection import train_test_split\n",
    "X90,X10,y90,y10 = train_test_split(plantvillages,stratify,stratify=stratify,test_size=0.1)\n",
    "\n",
    "#X10 is for training and some of images from X90 will be used for testing\n",
    "a = pd.DataFrame(X10)\n",
    "b = pd.DataFrame(docsdict)\n",
    "c = a.append(b)\n",
    "print(\"Village leaves:\",len(a),\"Docs leaves:\",len(b),\"Combined leaves for training:\",len(c))\n",
    "# c.to_csv(base_path+'/10%VillwithDoccombined.txt',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkOBEm9TUE0a"
   },
   "outputs": [],
   "source": [
    "Training_Dict = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdlmCUbwYYe0"
   },
   "source": [
    "NOw testing dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2fLHvEctWtX"
   },
   "outputs": [],
   "source": [
    "villimageloc= []\n",
    "for i in X90:\n",
    "  villimageloc.append(str(i.split(',')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "id0oEaWrtq_K",
    "outputId": "40b3197e-e233-4ca9-978d-d98dfadb84b5"
   },
   "outputs": [],
   "source": [
    "len(villimageloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bUy51OsRWQl3",
    "outputId": "f3e92cac-60d6-484e-b275-9e566a9dc6c5"
   },
   "outputs": [],
   "source": [
    "# shuffle = list(np.random.randint(0,923,92))\n",
    "# sampletestdocs = testdocs[shuffle]\n",
    "\n",
    "dftestdocs10 = pd.DataFrame(test_doc_images)\n",
    "dftestvills = pd.DataFrame(villimageloc)\n",
    "dftestvillss10 = dftestvills.sample(frac=0.005)\n",
    "\n",
    "finalTest = dftestdocs10.append(dftestvillss10)\n",
    "\n",
    "print(\"docs images are\",len(dftestdocs10),\" with Doc leaves are\",len(testdocs),\"and Vilages leaves cum images are\",len(dftestvillss10),\"with total images\",len(finalTest),\"THUS Combined leaves for final Testing:\",str(len(testdocs)+len(dftestvillss10)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KudbnfyP-RXt"
   },
   "outputs": [],
   "source": [
    "\n",
    "#FUNCTION for just image list\n",
    "ImgList = []\n",
    "ClassList=[]\n",
    "for i in c[0]:\n",
    "  ImgList.append(i.split(',')[0])\n",
    "  ClassList.append(i.split(',')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_3yFWww--qFx",
    "outputId": "9790ee02-84bb-4eb0-e314-c8a18c2dae64"
   },
   "outputs": [],
   "source": [
    "print(\"TRAINING DATASET HAS:\")\n",
    "print()\n",
    "pd.DataFrame(ClassList).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0tRbOeVK1EW"
   },
   "source": [
    "TXT files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68AVjTlxK29z"
   },
   "outputs": [],
   "source": [
    "Training_Dict[0].to_csv('training.txt',header=False,index=False)\n",
    "finalTest[0].to_csv('testing.txt',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvrZi3ax_18H"
   },
   "source": [
    "#Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCCTdhtycy_x"
   },
   "outputs": [],
   "source": [
    "with open(config_output_filename, 'rb') as f_in:\n",
    "\tC = pickle.load(f_in)\n",
    "\n",
    "# turn off any data augmentation at test time\n",
    "C.use_horizontal_flips = True\n",
    "C.use_vertical_flips = True\n",
    "C.rot_90 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swfbPvf1lEj0",
    "outputId": "23dc9d4d-5fde-4e2c-efe9-0fffd0d50013"
   },
   "outputs": [],
   "source": [
    "# This step will spend some time to load the data      \n",
    "\n",
    "st = time.time()\n",
    "train_imgs, classes_count, class_mapping = get_data('training.txt')\n",
    "print()\n",
    "print('%0.2f mins spent to load the data' % ((time.time()-st)/60) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7S4yjj4RxlW"
   },
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-8-UJ8ORerC",
    "outputId": "6f63bee0-95c6-48f0-c70b-85c953aaf0fe"
   },
   "outputs": [],
   "source": [
    "if 'bg' not in classes_count:\n",
    "\tclasses_count['bg'] = 0\n",
    "\tclass_mapping['bg'] = len(class_mapping)\n",
    "C.class_mapping = class_mapping\n",
    "\n",
    "print('Training images per class:')\n",
    "pprint.pprint(classes_count)\n",
    "print('Num classes (including bg) = {}'.format(len(classes_count)))\n",
    "print(class_mapping)\n",
    "\n",
    "# Save the configuration\n",
    "with open(config_output_filename, 'wb') as config_f:\n",
    "\tpickle.dump(C,config_f)\n",
    "\tprint('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(config_output_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jbYK4GhaRnfq",
    "outputId": "eead427d-53a6-413c-c8d7-04a4e637bbb8"
   },
   "outputs": [],
   "source": [
    "# Shuffle the images with seed\n",
    "random.seed(8)\n",
    "random.shuffle(train_imgs)\n",
    "\n",
    "print('Num train samples (leaves) {}'.format(len(train_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vm3O9FhyRqUx"
   },
   "outputs": [],
   "source": [
    "# Get train data generator which generate X, Y, image_data\n",
    "data_gen_train = get_anchor_gt(train_imgs, C, get_img_output_length, mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqF_D0ohR0Bq"
   },
   "outputs": [],
   "source": [
    "X, Y, image_data, debug_img, debug_num_pos = next(data_gen_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIBllD87R2Hn",
    "outputId": "7dc0bce6-07ed-4ed1-a0e5-a88c731b213b"
   },
   "outputs": [],
   "source": [
    "print('Original image: height=%d width=%d'%(image_data['height'], image_data['width']))\n",
    "print('Resized image:  height=%d width=%d C.im_size=%d'%(X.shape[1], X.shape[2], C.im_size))\n",
    "print('Feature map size: height=%d width=%d C.rpn_stride=%d'%(Y[0].shape[1], Y[0].shape[2], C.rpn_stride))\n",
    "print('input image reshaped',X.shape)\n",
    "print(str(len(Y))+\" includes 'y_rpn_cls' and 'y_rpn_regr'\")\n",
    "print('Shape of y_rpn_cls {}'.format(Y[0].shape))\n",
    "print('Shape of y_rpn_regr {}'.format(Y[1].shape))\n",
    "print(image_data)\n",
    "\n",
    "print('Number of positive anchors for this image: %d' % (debug_num_pos))\n",
    "if debug_num_pos==0:\n",
    "    gt_x1, gt_x2 = image_data['bboxes'][0]['x1']*(X.shape[2]/image_data['height']), image_data['bboxes'][0]['x2']*(X.shape[2]/image_data['height'])\n",
    "    gt_y1, gt_y2 = image_data['bboxes'][0]['y1']*(X.shape[1]/image_data['width']), image_data['bboxes'][0]['y2']*(X.shape[1]/image_data['width'])\n",
    "    gt_x1, gt_y1, gt_x2, gt_y2 = int(gt_x1), int(gt_y1), int(gt_x2), int(gt_y2)\n",
    "\n",
    "    img = debug_img.copy()\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    color = (0, 255, 0)\n",
    "    cv2.putText(img, 'gt bbox', (gt_x1, gt_y1-5), cv2.FONT_HERSHEY_DUPLEX, 0.7, color, 1)\n",
    "    cv2.rectangle(img, (gt_x1, gt_y1), (gt_x2, gt_y2), color, 2)\n",
    "    cv2.circle(img, (int((gt_x1+gt_x2)/2), int((gt_y1+gt_y2)/2)), 3, color, -1)\n",
    "\n",
    "    plt.grid()\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "else:\n",
    "    cls = Y[0][0]                   #y_rpn_cls = Y[0] , \n",
    "                                    #first element of y_rpn_cls = Y[0][0]\n",
    "    pos_cls = np.where(cls==1)\n",
    "    print(pos_cls)\n",
    "    regr = Y[1][0]                  #y_rpn_regr = Y[1]\n",
    "                                    ##first element of y_rpn_cls = Y[1][0]\n",
    "    pos_regr = np.where(regr==1)\n",
    "    print(pos_regr)\n",
    "    print('y_rpn_cls for possible pos anchor: {}'.format(cls[pos_cls[0][0],pos_cls[1][0],:]))\n",
    "    print('y_rpn_regr for positive anchor: {}'.format(regr[pos_regr[0][0],pos_regr[1][0],:]))\n",
    "\n",
    "    gt_x1, gt_x2 = image_data['bboxes'][0]['x1']*(X.shape[2]/image_data['width']), image_data['bboxes'][0]['x2']*(X.shape[2]/image_data['width'])\n",
    "    gt_y1, gt_y2 = image_data['bboxes'][0]['y1']*(X.shape[1]/image_data['height']), image_data['bboxes'][0]['y2']*(X.shape[1]/image_data['height'])\n",
    "    gt_x1, gt_y1, gt_x2, gt_y2 = int(gt_x1), int(gt_y1), int(gt_x2), int(gt_y2)\n",
    "\n",
    "    img = debug_img.copy()\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    color = (0, 255, 0)\n",
    "    #   cv2.putText(img, 'gt bbox', (gt_x1, gt_y1-5), cv2.FONT_HERSHEY_DUPLEX, 0.7, color, 1)\n",
    "    cv2.rectangle(img, (gt_x1, gt_y1), (gt_x2, gt_y2), color, 2)\n",
    "    cv2.circle(img, (int((gt_x1+gt_x2)/2), int((gt_y1+gt_y2)/2)), 3, color, -1)\n",
    "\n",
    "    # Add text\n",
    "    textLabel = 'gt bbox'\n",
    "    (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,0.5,1)\n",
    "    textOrg = (gt_x1, gt_y1+5)\n",
    "    cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 2)\n",
    "    cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)\n",
    "    cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 0.5, (0, 0, 0), 1)\n",
    "\n",
    "    # Draw positive anchors according to the y_rpn_regr\n",
    "    for i in range(debug_num_pos):\n",
    "\n",
    "        color = (100+i*(155/4), 0, 100+i*(155/4))\n",
    "\n",
    "        idx = pos_regr[2][i*4]/4\n",
    "        anchor_size = C.anchor_box_scales[int(idx/3)]\n",
    "        anchor_ratio = C.anchor_box_ratios[2-int((idx+1)%3)]\n",
    "\n",
    "        center = (pos_regr[1][i*4]*C.rpn_stride, pos_regr[0][i*4]*C.rpn_stride)\n",
    "        print('Center position of positive anchor: ', center)\n",
    "        cv2.circle(img, center, 3, color, -1)\n",
    "        anc_w, anc_h = anchor_size*anchor_ratio[0], anchor_size*anchor_ratio[1]\n",
    "        cv2.rectangle(img, (center[0]-int(anc_w/2), center[1]-int(anc_h/2)), (center[0]+int(anc_w/2), center[1]+int(anc_h/2)), color, 2)\n",
    "#         cv2.putText(img, 'pos anchor bbox '+str(i+1), (center[0]-int(anc_w/2), center[1]-int(anc_h/2)-5), cv2.FONT_HERSHEY_DUPLEX, 0.5, color, 1)\n",
    "\n",
    "print('Green bboxes is ground-truth bbox. Others are positive anchors')\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.grid()\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2unVYLsR4B3"
   },
   "outputs": [],
   "source": [
    "input_shape_img = (None, None, 3)\n",
    "\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(None, 4))\n",
    "\n",
    "# define the base network (VGG here, can be Resnet50, Inception, etc)\n",
    "shared_layers = nn_base(img_input, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsOy28xwR8on",
    "outputId": "cef02a38-07cd-4d03-be57-ce301e447dad"
   },
   "outputs": [],
   "source": [
    "# define the RPN, built on the base layers\n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios) # 9\n",
    "rpn = rpn_layer(shared_layers, num_anchors)\n",
    "\n",
    "classifier = classifier_layer(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count))\n",
    "\n",
    "model_rpn = Model(img_input, rpn[:2])\n",
    "model_classifier = Model([img_input, roi_input], classifier)\n",
    "\n",
    "# this is a model that holds both the RPN and the classifier, used to load/save weights for the models\n",
    "model_all = Model([img_input, roi_input], rpn[:2] + classifier)\n",
    "\n",
    "# Because the google colab can only run the session several hours one time (then you need to connect again), \n",
    "# we need to save the model and load the model to continue training\n",
    "if not os.path.isfile(C.model_path):\n",
    "    #If this is the begin of the training, load the pre-traind base network such as vgg-16\n",
    "    try:\n",
    "        print('This is the first time of your training')\n",
    "        print('loading weights from {}'.format(C.base_net_weights))\n",
    "        model_rpn.load_weights(C.base_net_weights, by_name=True)\n",
    "        model_classifier.load_weights(C.base_net_weights, by_name=True)\n",
    "    except:\n",
    "        print('load pretrained model weights')\n",
    "            \n",
    "    \n",
    "    # Create the record.csv file to record losses, acc and mAP\n",
    "    record_df = pd.DataFrame(columns=['mean_overlapping_bboxes', 'class_acc', 'loss_rpn_cls', 'loss_rpn_regr', 'loss_class_cls', 'loss_class_regr', 'curr_loss', 'elapsed_time', 'mAP'])\n",
    "else:\n",
    "    # If this is a continued training, load the trained model from before\n",
    "    print('Continue training based on previous trained model')\n",
    "    print('Loading weights from {}'.format(C.model_path))\n",
    "    model_rpn.load_weights(C.model_path, by_name=True)\n",
    "    model_classifier.load_weights(C.model_path, by_name=True)\n",
    "    \n",
    "    # Load the records\n",
    "    record_df = pd.read_csv(record_path)\n",
    "\n",
    "    r_mean_overlapping_bboxes = record_df['mean_overlapping_bboxes']\n",
    "    r_class_acc = record_df['class_acc']\n",
    "    r_loss_rpn_cls = record_df['loss_rpn_cls']\n",
    "    r_loss_rpn_regr = record_df['loss_rpn_regr']\n",
    "    r_loss_class_cls = record_df['loss_class_cls']\n",
    "    r_loss_class_regr = record_df['loss_class_regr']\n",
    "    r_curr_loss = record_df['curr_loss']\n",
    "    r_elapsed_time = record_df['elapsed_time']\n",
    "    r_mAP = record_df['mAP']\n",
    "\n",
    "    print('Already train %dK batches'% (len(record_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlMnWRTDPd7d"
   },
   "source": [
    "#Training  unhash to train\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4q0H3AJSBUP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEvLrmtXzWZ0"
   },
   "source": [
    "#Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKa9JSBvc3I_"
   },
   "outputs": [],
   "source": [
    "def format_img_size(img, C):\n",
    "\t\"\"\" formats the image size based on config \"\"\"\n",
    "\timg_min_side = float(C.im_size)\n",
    "\t(height,width,_) = img.shape\n",
    "\t\t\n",
    "\tif width <= height:\n",
    "\t\tratio = img_min_side/width\n",
    "\t\tnew_height = int(ratio * height)\n",
    "\t\tnew_width = int(img_min_side)\n",
    "\telse:\n",
    "\t\tratio = img_min_side/height\n",
    "\t\tnew_width = int(ratio * width)\n",
    "\t\tnew_height = int(img_min_side)\n",
    "\timg = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "\treturn img, ratio\t\n",
    "\n",
    "def format_img_channels(img, C):\n",
    "\t\"\"\" formats the image channels based on config \"\"\"\n",
    "\timg = img[:, :, (2, 1, 0)]\n",
    "\timg = img.astype(np.float32)\n",
    "\timg[:, :, 0] -= C.img_channel_mean[0]\n",
    "\timg[:, :, 1] -= C.img_channel_mean[1]\n",
    "\timg[:, :, 2] -= C.img_channel_mean[2]\n",
    "\timg /= C.img_scaling_factor\n",
    "\timg = np.transpose(img, (2, 0, 1))\n",
    "\timg = np.expand_dims(img, axis=0)\n",
    "\treturn img\n",
    "\n",
    "def format_img(img, C):\n",
    "\t\"\"\" formats an image for model prediction based on config \"\"\"\n",
    "\timg, ratio = format_img_size(img, C)\n",
    "\timg = format_img_channels(img, C)\n",
    "\treturn img, ratio\n",
    "\n",
    "# Method to transform the coordinates of the bounding box to its original size\n",
    "def get_real_coordinates(ratio, x1, y1, x2, y2):\n",
    "\n",
    "\treal_x1 = int(round(x1 // ratio))\n",
    "\treal_y1 = int(round(y1 // ratio))\n",
    "\treal_x2 = int(round(x2 // ratio))\n",
    "\treal_y2 = int(round(y2 // ratio))\n",
    "\n",
    "\treturn (real_x1, real_y1, real_x2 ,real_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bikGLESLB-8-"
   },
   "source": [
    "HERE we'll be calling all our so far functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mir1mRqndLWg",
    "outputId": "d1f45d4d-4519-4455-de8e-a3f01cf70b25"
   },
   "outputs": [],
   "source": [
    "\n",
    "num_features = 512\n",
    "\n",
    "input_shape_img = (None, None, 3)\n",
    "input_shape_features = (None, None, num_features)\n",
    "\n",
    "# Making sure of the Input params like shape of input images, ROIs  and feature Maps for the CNN to pass into\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(C.num_rois, 4))\n",
    "feature_map_input = Input(shape=input_shape_features)\n",
    "\n",
    "# define the base network (VGG-16 here) \n",
    "shared_layers = nn_base(img_input, trainable=True)                                                          # First function called\n",
    "\n",
    "# define the RPN, built on the base layers \n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n",
    "rpn_layers = rpn_layer(shared_layers, num_anchors)                                                          # calling rpn function\n",
    "\n",
    "classifier = classifier_layer(feature_map_input, roi_input, C.num_rois, nb_classes=len(C.class_mapping))    # classifier_layer\n",
    "\n",
    "model_rpn = Model(img_input, rpn_layers)\n",
    "model_classifier_only = Model([feature_map_input, roi_input], classifier)                                   # Path 1\n",
    "\n",
    "model_classifier = Model([feature_map_input, roi_input], classifier)                                        # Path 2\n",
    "\n",
    "print('Loading weights from {}'.format(C.model_path))\n",
    "model_rpn.load_weights(C.model_path, by_name=True)\n",
    "model_classifier.load_weights(C.model_path, by_name=True)\n",
    "\n",
    "model_rpn.compile(optimizer='sgd', loss='mse')\n",
    "model_classifier.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkgg4sAddQvS",
    "outputId": "6c632148-4a8a-4450-cdd4-349cbfe540cc"
   },
   "outputs": [],
   "source": [
    "# Switch key value for class mapping\n",
    "class_mapping = C.class_mapping\n",
    "class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "print(class_mapping)\n",
    "class_to_color = {class_mapping[v]: np.random.randint(0, 255, 3) for v in class_mapping}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrHnBys9MugU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9iV9x-2MuyU"
   },
   "source": [
    "#TEST NOW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXS9TIM2_Jbi"
   },
   "outputs": [],
   "source": [
    "test_img_df = pd.read_csv('testing.txt',names=['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "-TUcwlDh_Mj5",
    "outputId": "37708435-67f3-41bc-9c2f-d279e5f8bd7f"
   },
   "outputs": [],
   "source": [
    "test_img_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kn9FFyu2_SFa",
    "outputId": "fd64fefe-d4b4-447b-b354-f3d58976ffb8"
   },
   "outputs": [],
   "source": [
    "len(list(test_img_df['0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ffUfPsJl_X-n",
    "outputId": "49fda884-7f14-4df7-a493-48bcba0bd554"
   },
   "outputs": [],
   "source": [
    "test_img_df['0'][188]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "toAYSObqMymO",
    "outputId": "49a47463-e8ed-47db-8272-d7ee45ad4e79"
   },
   "outputs": [],
   "source": [
    "village = pd.read_csv('drive/MyDrive/FRCNN/keras-frcnn-master/keras_frcnn/new/plantvillage.txt')\n",
    "docs     = pd.read_csv('drive/MyDrive/FRCNN/keras-frcnn-master/keras_frcnn/new/plantdocs.txt')\n",
    "allcombined= pd.read_csv('drive/MyDrive/FRCNN/keras-frcnn-master/keras_frcnn/new/PlantVillageDoc.txt')\n",
    "vill10doc= pd.read_csv('drive/MyDrive/FRCNN/keras-frcnn-master/keras_frcnn/new/10%VillwithDoccombined.txt')\n",
    "\n",
    "print('village:',len(village),'docs:',len(docs),'allcombined:',len(allcombined),'vill10doc:',len(vill10doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hZSH52jY_AR"
   },
   "outputs": [],
   "source": [
    "testingdict=[]\n",
    "\n",
    "for i in allimages['parsedxml']:\n",
    "  imgloc= i.split(',')[0]\n",
    "  if imgloc in list(test_img_df['0']):\n",
    "    testingdict.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "il8iGYsTcL3-",
    "outputId": "e94e3089-00c3-4e34-8639-452c493a90c1"
   },
   "outputs": [],
   "source": [
    "len(testingdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFWIVEUYcp6Z"
   },
   "outputs": [],
   "source": [
    "testmap = pd.DataFrame(testingdict)\n",
    "testmap.to_csv('testmap.txt',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pS9Bx3wizH01",
    "outputId": "03bfb316-faa1-4079-d41c-ccb31dfc236f"
   },
   "outputs": [],
   "source": [
    "# # This might takes a while to parser the data\n",
    "test_imgs_dict, _, _ = get_data('testmap.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_tVd532v7xA"
   },
   "outputs": [],
   "source": [
    "imgs_path = list(test_img_df['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CJ_qC87PR7Su",
    "outputId": "8213825a-4d79-4a1e-abb5-86b55e850735"
   },
   "outputs": [],
   "source": [
    "len(imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xr_QVWnUd6hz",
    "outputId": "3171e098-7c49-43ee-b101-ca589e9c200a"
   },
   "outputs": [],
   "source": [
    "\n",
    "test_imgs_path = imgs_path\n",
    "a,b,c,d,e =[],[],[],[],[]\n",
    "# If the box classification value is less than this, we ignore this box\n",
    "bbox_threshold = 0.7\n",
    "\n",
    "for idx, img_name in enumerate(test_imgs_path):\n",
    "    if not img_name.lower().endswith(('.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff')):\n",
    "        continue\n",
    "    print(idx,img_name)\n",
    "    a.append(img_name)\n",
    "    st = time.time()\n",
    "    filepath = img_name\n",
    "    # print(filepath)\n",
    "    # img = cv2.imread(str(filepath))\n",
    "    # print(img.shape)\n",
    "    img = cv2.imread(filepath)\n",
    "    X, ratio = format_img(img, C)\n",
    "    \n",
    "    X = np.transpose(X, (0, 2, 3, 1))\n",
    "\n",
    "    # get output layer Y1, Y2 from the RPN and the feature maps F\n",
    "    # Y1: y_rpn_cls\n",
    "    # Y2: y_rpn_regr\n",
    "    [Y1, Y2, F] = model_rpn.predict(X)\n",
    "\n",
    "    # Get bboxes by applying NMS \n",
    "    # R.shape = (300, 4)\n",
    "    R = rpn_to_roi(Y1, Y2, C, K.image_data_format(), overlap_thresh=0.7)\n",
    "\n",
    "    # convert from (x1,y1,x2,y2) to (x,y,w,h)\n",
    "    R[:, 2] -= R[:, 0]\n",
    "    R[:, 3] -= R[:, 1]\n",
    "\n",
    "    # apply the spatial pyramid pooling to the proposed regions\n",
    "    bboxes = {}\n",
    "    probs = {}\n",
    "\n",
    "    for jk in range(R.shape[0]//C.num_rois + 1):\n",
    "        ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n",
    "        if ROIs.shape[1] == 0:\n",
    "            break\n",
    "\n",
    "        if jk == R.shape[0]//C.num_rois:\n",
    "            #pad R\n",
    "            curr_shape = ROIs.shape\n",
    "            target_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n",
    "            ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n",
    "            ROIs_padded[:, :curr_shape[1], :] = ROIs\n",
    "            ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n",
    "            ROIs = ROIs_padded\n",
    "\n",
    "        [P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n",
    "\n",
    "        # Calculate bboxes coordinates on resized image\n",
    "        for ii in range(P_cls.shape[1]):\n",
    "            # Ignore 'bg' class\n",
    "            if np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n",
    "                continue\n",
    "\n",
    "            cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n",
    "\n",
    "            if cls_name not in bboxes:\n",
    "                bboxes[cls_name] = []\n",
    "                probs[cls_name] = []\n",
    "\n",
    "            (x, y, w, h) = ROIs[0, ii, :]\n",
    "\n",
    "            cls_num = np.argmax(P_cls[0, ii, :])\n",
    "            try:\n",
    "                (tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n",
    "                tx /= C.classifier_regr_std[0]\n",
    "                ty /= C.classifier_regr_std[1]\n",
    "                tw /= C.classifier_regr_std[2]\n",
    "                th /= C.classifier_regr_std[3]\n",
    "                x, y, w, h = apply_regr(x, y, w, h, tx, ty, tw, th)\n",
    "            except:\n",
    "                pass\n",
    "            bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n",
    "            probs[cls_name].append(np.max(P_cls[0, ii, :]))\n",
    "\n",
    "    all_dets = []\n",
    "\n",
    "    for key in bboxes:\n",
    "        bbox = np.array(bboxes[key])\n",
    "\n",
    "        new_boxes, new_probs = non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.2)\n",
    "        for jk in range(new_boxes.shape[0]):\n",
    "            (x1, y1, x2, y2) = new_boxes[jk,:]\n",
    "\n",
    "            # Calculate real coordinates on original image\n",
    "            (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)\n",
    "\n",
    "            cv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),4)\n",
    "\n",
    "            textLabel = '{}: {}'.format(key,int(100*new_probs[jk]))\n",
    "            all_dets.append((key,100*new_probs[jk]))\n",
    "\n",
    "            (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)\n",
    "            textOrg = (real_x1, real_y1-0)\n",
    "\n",
    "            cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 1)\n",
    "            cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)\n",
    "            cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)\n",
    "            \n",
    "            \n",
    "            if len(all_dets)==0:\n",
    "              b.append([idx,'bg'])\n",
    "            else:\n",
    "              b.append([idx,all_dets])\n",
    "\n",
    "    if len(all_dets)==0:\n",
    "              c.append([idx,[('bg',100)]])\n",
    "    else:\n",
    "              c.append([idx,all_dets])\n",
    "\n",
    "    d.append(len(all_dets))\n",
    "\n",
    "    print('Elapsed time = {}'.format(time.time() - st))\n",
    "    print(all_dets)\n",
    "    print(\"Total no. of diseases in this leaf image\", len(all_dets))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.grid()\n",
    "    plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qo5caIVxhr8b",
    "outputId": "e9423b87-5bda-40d9-bf49-916fd9e69f7c"
   },
   "outputs": [],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOClutOGOMzv",
    "outputId": "d5f2d5f2-f234-4278-bba7-a99a37857b94"
   },
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZ-5-qSuOWAg",
    "outputId": "60d978cd-978d-425b-d39c-f324fedf6e7b"
   },
   "outputs": [],
   "source": [
    "len(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEDzn17-7hbc"
   },
   "source": [
    "# Y_pred and Y_true for classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFaqTs23OYg7",
    "outputId": "ec12e5d1-b70b-4cb9-9778-b5a5f68da68d"
   },
   "outputs": [],
   "source": [
    "#function for y_pred to make single item list free\n",
    "y=[]\n",
    "for i in c:\n",
    "  if len(i[1])==1:\n",
    "    y.append(i[1][0][0])\n",
    "  else:\n",
    "    listring=[]\n",
    "    for j in i[1]:\n",
    "      listring.append(j[0])\n",
    "    y.append(listring)\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIhld2OjHCnc",
    "outputId": "8cb85820-b404-46d9-f523-6e80a9ea8b7c"
   },
   "outputs": [],
   "source": [
    "y_pred = y.copy()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2rcMBIgQX8aI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAmu1_uIHWnf"
   },
   "source": [
    "Y_pre done next Y_tru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tofnYNNO7vUs"
   },
   "source": [
    "Now Y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKQmTK5ZyOqP",
    "outputId": "281f9705-29a7-4c0b-b265-f719e53718cd"
   },
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FVSsr4sjHu9O",
    "outputId": "b17a1c1f-6239-431e-9770-cc9857c5a799"
   },
   "outputs": [],
   "source": [
    "y_t=[]\n",
    "for i in a:\n",
    "  if i in list(df['filepath']):\n",
    "    idximg = list(df['filepath']).index(i)\n",
    "    classes = df['y_true'][idximg]\n",
    "    if classes[0] =='[':\n",
    "      classlist=[]\n",
    "      strings = classes.split(',')\n",
    "      for i in strings:\n",
    "        try:\n",
    "          i= i.replace(\"'\",\"\")\n",
    "          i= i.replace(\" \",\"\")\n",
    "          i= i.replace(\"[\",\"\")\n",
    "          i= i.replace(\"]\",\"\")\n",
    "          classlist.append(i)\n",
    "        except:\n",
    "          pass\n",
    "      \n",
    "      y_t.append(classlist)\n",
    "    else:\n",
    "      y_t.append(classes)\n",
    "len(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LlbnXKB_-BP_",
    "outputId": "966a6b1a-4887-499b-91f6-982dabc8dbeb"
   },
   "outputs": [],
   "source": [
    "y_true = y_t.copy()\n",
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mh9xDBMb3edY"
   },
   "source": [
    "Y true done Now make sure both are having same length then classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "296jBExCwBBm"
   },
   "source": [
    "#Precise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vA79matuumd-"
   },
   "outputs": [],
   "source": [
    "def mml (y_true,y_pred):\n",
    "  y_predicted=[]\n",
    "\n",
    "  for j,i in enumerate(y_pred):\n",
    "    if type(i)==list:                           #pred vs true\n",
    "\n",
    "        if type(y_pred[j])==list:                        #1 vs 1      \n",
    "\n",
    "            if len(y_pred[j])-len(i)>0:                           #GREATER\n",
    "                      i.extend(['bg'] * (len(y_pred[j])-len(i)))\n",
    "                      y_predicted.append(i)\n",
    "\n",
    "            else:                                                 #LESSER or EQUAL\n",
    "                    while len(y_pred[j])-len(i) !=0:\n",
    "                      i.pop(-1)\n",
    "                    y_predicted.append(i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        else:                                           #1 vs 0\n",
    "            while len(i)!=1:\n",
    "              i.pop(-1)\n",
    "            y_predicted.append(i)\n",
    "      \n",
    "    elif type(i)==str and type(y_pred[j])==list:    #0 vs 1\n",
    "\n",
    "        extendlist=[i]\n",
    "        extendlist.extend(['bg'] * (len(y_true[j])-1))\n",
    "        y_predicted.append(extendlist)\n",
    "\n",
    "      \n",
    "    else:                                           #0 vs 0    \n",
    "      y_predicted.append(i)\n",
    "  return y_predicted\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JgNVxI5v-1d"
   },
   "outputs": [],
   "source": [
    "def multitolinear(multilist):\n",
    "  linear =[]\n",
    "  for i in multilist:\n",
    "    if type(i)==list :\n",
    "      for j in i:\n",
    "        linear.append(j)\n",
    "    else:\n",
    "      linear.append(i)\n",
    "  return linear\n",
    "\n",
    "def makesuresameleaves(y_true,y_pred):\n",
    "  y_predicted =[]\n",
    "  for j,i in enumerate(y_pred):\n",
    "      if type(i)==list:                             #made sure that i is a list        \n",
    "\n",
    "          #ASSIGN listlen\n",
    "          if type(y_true[j])==list:\n",
    "            listlen = len(y_true[j])\n",
    "          else:\n",
    "            listlen=1\n",
    "          \n",
    "          #COMPARE WITH listlen\n",
    "          if listlen-len(i)==0:                             #EQUAL\n",
    "              y_predicted.append(i)\n",
    "\n",
    "          else:\n",
    "              if listlen-len(i)>0:                           #GREATER\n",
    "                    i.extend(['bg'] * (listlen-len(i)))\n",
    "                    y_predicted.append(i)\n",
    "\n",
    "              else:                                          #LESSER\n",
    "                  while listlen-len(i) !=0:\n",
    "                    i.pop(-1)\n",
    "                  y_predicted.append(i)\n",
    "      elif type(i)==str and type(y_true[j])==list:    \n",
    "        extendlist=[str(i)]\n",
    "        extendlist.extend(['bg'] * (len(y_true[j])-1))\n",
    "        y_predicted.append(extendlist)\n",
    "\n",
    "      else:\n",
    "        y_predicted.append(i)                      \n",
    "                                                        \n",
    "  # y_true_linear = multitolinear(y_true)\n",
    "  # y_pred_linear = multitolinear(y_predicted)\n",
    "  # return y_true_linear,y_pred_linear\n",
    "  return y_predicted\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__5CsYZRe5z2",
    "outputId": "4f32211f-941f-4f7c-9b61-e4c0947c8e4c"
   },
   "outputs": [],
   "source": [
    "print(len(y_true),len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RO_w6O9CYsmB"
   },
   "outputs": [],
   "source": [
    "x1 = multitolinear(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdEPS1zsY7PR",
    "outputId": "6d98156c-4d1a-4da0-a9f9-12e095ebf047"
   },
   "outputs": [],
   "source": [
    "print(x1)\n",
    "print(len(x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0ydgeXEzOSQ",
    "outputId": "7ea1eebd-7478-4dc3-de4e-011fcf334173"
   },
   "outputs": [],
   "source": [
    "y3= makesuresameleaves(y_true,y_pred)\n",
    "print(len(y3))\n",
    "\n",
    "print(y3)\n",
    "x3 = multitolinear(y3)\n",
    "print(len(x3))\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpWHec8m4Xqm"
   },
   "outputs": [],
   "source": [
    "y_true = x1.copy()\n",
    "y_pred = x3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ImiIHkgG4sSE",
    "outputId": "9e89762a-8b63-4536-a826-0323ef7fa0cd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUYfRB6wcjCs",
    "outputId": "6fd3e3db-8fbb-44c1-9510-38d215fd65c7"
   },
   "outputs": [],
   "source": [
    "target_names= list(class_mapping.values())\n",
    "print(target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfZC7LgMz9EX"
   },
   "source": [
    "#Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRNsWuSRC0JC"
   },
   "source": [
    "Plotting graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QLcrqPcSc1A1",
    "outputId": "6c17a492-b34e-4dd1-d82c-b57f0eaf0561"
   },
   "outputs": [],
   "source": [
    "# Load the records\n",
    "record_df = pd.read_csv(C.record_path)\n",
    "\n",
    "r_epochs = len(record_df)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['mean_overlapping_bboxes'], 'r')\n",
    "plt.title('mean_overlapping_bboxes')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['class_acc'], 'r')\n",
    "plt.title('class_acc')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_cls'], 'r')\n",
    "plt.title('loss_rpn_cls')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_regr'], 'r')\n",
    "plt.title('loss_rpn_regr')\n",
    "plt.show()\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['loss_class_cls'], 'r')\n",
    "plt.title('loss_class_cls')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['loss_class_regr'], 'r')\n",
    "plt.title('loss_class_regr')\n",
    "plt.show()\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['curr_loss'], 'r')\n",
    "plt.title('total_loss')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.arange(0, r_epochs), record_df['elapsed_time'], 'r')\n",
    "plt.title('elapsed_time')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSj58F8Oi9rZ"
   },
   "source": [
    "Measure mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYxdt_eid-PW"
   },
   "outputs": [],
   "source": [
    "def get_map(pred, gt, f):\n",
    "\tT = {}\n",
    "\tP = {}\n",
    "\tfx, fy = f\n",
    "\n",
    "\tfor bbox in gt:\n",
    "\t\tbbox['bbox_matched'] = False\n",
    "\n",
    "\tpred_probs = np.array([s['prob'] for s in pred])\n",
    "\tbox_idx_sorted_by_prob = np.argsort(pred_probs)[::-1]\n",
    "\n",
    "\tfor box_idx in box_idx_sorted_by_prob:\n",
    "\t\tpred_box = pred[box_idx]\n",
    "\t\tpred_class = pred_box['class']\n",
    "\t\tpred_x1 = pred_box['x1']\n",
    "\t\tpred_x2 = pred_box['x2']\n",
    "\t\tpred_y1 = pred_box['y1']\n",
    "\t\tpred_y2 = pred_box['y2']\n",
    "\t\tpred_prob = pred_box['prob']\n",
    "\t\tif pred_class not in P:\n",
    "\t\t\tP[pred_class] = []\n",
    "\t\t\tT[pred_class] = []\n",
    "\t\tP[pred_class].append(pred_prob)\n",
    "\t\tfound_match = False\n",
    "\n",
    "\t\tfor gt_box in gt:\n",
    "\t\t\tgt_class = gt_box['class']\n",
    "\t\t\tgt_x1 = gt_box['x1']/fx\n",
    "\t\t\tgt_x2 = gt_box['x2']/fx\n",
    "\t\t\tgt_y1 = gt_box['y1']/fy\n",
    "\t\t\tgt_y2 = gt_box['y2']/fy\n",
    "\t\t\tgt_seen = gt_box['bbox_matched']\n",
    "\t\t\tif gt_class != pred_class:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif gt_seen:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tiou_map = iou((pred_x1, pred_y1, pred_x2, pred_y2), (gt_x1, gt_y1, gt_x2, gt_y2))\n",
    "\t\t\tif iou_map >= 0.5:\n",
    "\t\t\t\tfound_match = True\n",
    "\t\t\t\tgt_box['bbox_matched'] = True\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\tT[pred_class].append(int(found_match))\n",
    "\n",
    "\tfor gt_box in gt:\n",
    "\t\tif not gt_box['bbox_matched']:# and not gt_box['difficult']:\n",
    "\t\t\tif gt_box['class'] not in P:\n",
    "\t\t\t\tP[gt_box['class']] = []\n",
    "\t\t\t\tT[gt_box['class']] = []\n",
    "\n",
    "\t\t\tT[gt_box['class']].append(1)\n",
    "\t\t\tP[gt_box['class']].append(0)\n",
    "\n",
    "\n",
    "\treturn T, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7leiffrLiv9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvjzSQ7DjBb7"
   },
   "outputs": [],
   "source": [
    "def format_img_map(img, C):\n",
    "\t\"\"\"Format image for mAP. Resize original image to C.im_size (300 in here)\n",
    "\n",
    "\tArgs:\n",
    "\t\timg: cv2 image\n",
    "\t\tC: config\n",
    "\n",
    "\tReturns:\n",
    "\t\timg: Scaled and normalized image with expanding dimension\n",
    "\t\tfx: ratio for width scaling\n",
    "\t\tfy: ratio for height scaling\n",
    "\t\"\"\"\n",
    "\n",
    "\timg_min_side = float(C.im_size)\n",
    "\t(height,width,_) = img.shape\n",
    "\t\n",
    "\tif width <= height:\n",
    "\t\tf = img_min_side/width\n",
    "\t\tnew_height = int(f * height)\n",
    "\t\tnew_width = int(img_min_side)\n",
    "\telse:\n",
    "\t\tf = img_min_side/height\n",
    "\t\tnew_width = int(f * width)\n",
    "\t\tnew_height = int(img_min_side)\n",
    "\tfx = width/float(new_width)\n",
    "\tfy = height/float(new_height)\n",
    "\timg = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "\t# Change image channel from BGR to RGB\n",
    "\timg = img[:, :, (2, 1, 0)]\n",
    "\timg = img.astype(np.float32)\n",
    "\timg[:, :, 0] -= C.img_channel_mean[0]\n",
    "\timg[:, :, 1] -= C.img_channel_mean[1]\n",
    "\timg[:, :, 2] -= C.img_channel_mean[2]\n",
    "\timg /= C.img_scaling_factor\n",
    "\t# Change img shape from (height, width, channel) to (channel, height, width)\n",
    "\timg = np.transpose(img, (2, 0, 1))\n",
    "\t# Expand one dimension at axis 0\n",
    "\t# img shape becames (1, channel, height, width)\n",
    "\timg = np.expand_dims(img, axis=0)\n",
    "\treturn img, fx, fy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6O5OdaMjFPa",
    "outputId": "6a2b0a29-3191-4908-96e7-f9962f6b1bed"
   },
   "outputs": [],
   "source": [
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIDtjJ4mb0tp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Su0PUaVcjHfG",
    "outputId": "3221e197-475c-410d-a8ec-97a6c222ba9b"
   },
   "outputs": [],
   "source": [
    "fil = []\n",
    "afinal=[]\n",
    "for i in b:\n",
    "    fil.append(i[0])\n",
    "for j in fil:\n",
    "    afinal.append(a[j])\n",
    "print(len(afinal))\n",
    "test_imgs=afinal\n",
    "test_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CaNEWh33lOJW",
    "outputId": "c01e3ceb-c42f-49f6-fc3a-56ac59a7a8c7"
   },
   "outputs": [],
   "source": [
    "T = {}\n",
    "P = {}\n",
    "mAPs = []\n",
    "for idx, img_data in enumerate(test_imgs_dict):\n",
    "    print('{}/{}'.format(idx,len(test_imgs)))\n",
    "    st = time.time()\n",
    "    filepath = img_data['filepath']\n",
    "\n",
    "    img = cv2.imread(filepath)\n",
    "\n",
    "    X, fx, fy = format_img_map(img, C)\n",
    "\n",
    "    # Change X (img) shape from (1, channel, height, width) to (1, height, width, channel)\n",
    "    X = np.transpose(X, (0, 2, 3, 1))\n",
    "\n",
    "    # get the feature maps and output from the RPN\n",
    "    [Y1, Y2, F] = model_rpn.predict(X)\n",
    "\n",
    "\n",
    "    R = rpn_to_roi(Y1, Y2, C, K.image_data_format(), overlap_thresh=0.7)\n",
    "\n",
    "    # convert from (x1,y1,x2,y2) to (x,y,w,h)\n",
    "    R[:, 2] -= R[:, 0]\n",
    "    R[:, 3] -= R[:, 1]\n",
    "\n",
    "    # apply the spatial pyramid pooling to the proposed regions\n",
    "    bboxes = {}\n",
    "    probs = {}\n",
    "\n",
    "    for jk in range(R.shape[0] // C.num_rois + 1):\n",
    "        ROIs = np.expand_dims(R[C.num_rois * jk:C.num_rois * (jk + 1), :], axis=0)\n",
    "        if ROIs.shape[1] == 0:\n",
    "            break\n",
    "\n",
    "        if jk == R.shape[0] // C.num_rois:\n",
    "            # pad R\n",
    "            curr_shape = ROIs.shape\n",
    "            target_shape = (curr_shape[0], C.num_rois, curr_shape[2])\n",
    "            ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n",
    "            ROIs_padded[:, :curr_shape[1], :] = ROIs\n",
    "            ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n",
    "            ROIs = ROIs_padded\n",
    "\n",
    "        [P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n",
    "\n",
    "        # Calculate all classes' bboxes coordinates on resized image (300, 400)\n",
    "        # Drop 'bg' classes bboxes\n",
    "        for ii in range(P_cls.shape[1]):\n",
    "\n",
    "            # If class name is 'bg', continue\n",
    "            if np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n",
    "                continue\n",
    "\n",
    "            # Get class name\n",
    "            cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n",
    "\n",
    "            if cls_name not in bboxes:\n",
    "                bboxes[cls_name] = []\n",
    "                probs[cls_name] = []\n",
    "\n",
    "            (x, y, w, h) = ROIs[0, ii, :]\n",
    "\n",
    "            cls_num = np.argmax(P_cls[0, ii, :])\n",
    "            try:\n",
    "                (tx, ty, tw, th) = P_regr[0, ii, 4 * cls_num:4 * (cls_num + 1)]\n",
    "                tx /= C.classifier_regr_std[0]\n",
    "                ty /= C.classifier_regr_std[1]\n",
    "                tw /= C.classifier_regr_std[2]\n",
    "                th /= C.classifier_regr_std[3]\n",
    "                x, y, w, h = roi_helpers.apply_regr(x, y, w, h, tx, ty, tw, th)\n",
    "            except:\n",
    "                pass\n",
    "            bboxes[cls_name].append([16 * x, 16 * y, 16 * (x + w), 16 * (y + h)])\n",
    "            probs[cls_name].append(np.max(P_cls[0, ii, :]))\n",
    "\n",
    "    all_dets = []\n",
    "\n",
    "    for key in bboxes:\n",
    "        bbox = np.array(bboxes[key])\n",
    "\n",
    "        # Apply non-max-suppression on final bboxes to get the output bounding box\n",
    "        new_boxes, new_probs = non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.5)\n",
    "        for jk in range(new_boxes.shape[0]):\n",
    "            (x1, y1, x2, y2) = new_boxes[jk, :]\n",
    "            det = {'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class': key, 'prob': new_probs[jk]}\n",
    "            all_dets.append(det)\n",
    "\n",
    "\n",
    "    print('Elapsed time = {}'.format(time.time() - st))\n",
    "    t, p = get_map(all_dets, img_data['bboxes'], (fx, fy))\n",
    "    for key in t.keys():\n",
    "        if key not in T:\n",
    "            T[key] = []\n",
    "            P[key] = []\n",
    "        T[key].extend(t[key])\n",
    "        P[key].extend(p[key])\n",
    "    all_aps = []\n",
    "    for key in T.keys():\n",
    "        ap = average_precision_score(T[key], P[key])\n",
    "        try :\n",
    "            int(ap)\n",
    "            print('{} AP: {}'.format(key, ap))\n",
    "            all_aps.append(ap)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        \n",
    "    print('mAP = {}'.format(np.mean(np.array(all_aps))))\n",
    "    mAPs.append(np.mean(np.array(all_aps)))\n",
    "    #print(T)\n",
    "    #print(P)\n",
    "    \n",
    "print()\n",
    "print('mean average precision:', np.mean(np.array(mAPs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1k0Cgn05n40M",
    "outputId": "f9d60095-b6ae-47e7-e469-e87b21356667"
   },
   "outputs": [],
   "source": [
    "mAP = [mAP for mAP in mAPs if str(mAP)!='nan']\n",
    "mean_average_prec = round(np.mean(np.array(mAP)), 3)\n",
    "record_df = pd.read_csv(record_path)\n",
    "print('After training %dk batches, the mean average precision is %0.3f'%(len(record_df), mean_average_prec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UQ_zJxrqhOh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEvokY7cLlUF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ys_upHart9si"
   },
   "source": [
    "#Testing Images from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvZvw7GrwALM"
   },
   "outputs": [],
   "source": [
    "#location of folder in which testing images are present\n",
    "\n",
    "testfolder = base_path+'/testfolder'\n",
    "test_imgs_path = os.listdir(testfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHpdBQLpv1L1"
   },
   "outputs": [],
   "source": [
    "# If the box classification value is less than this, we ignore this box\n",
    "bbox_threshold = 0.7\n",
    "\n",
    "for idx, img_name in enumerate(test_imgs_path):\n",
    "    if not img_name.lower().endswith(('.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff')):\n",
    "        continue\n",
    "    filepath = testfolder+'/'+img_name\n",
    "    print(idx,filepath)\n",
    "    a.append(filepath)\n",
    "    st = time.time()\n",
    "    img = cv2.imread(str(filepath))\n",
    "    print(img.shape)\n",
    "    X, ratio = format_img(img, C)\n",
    "    \n",
    "    X = np.transpose(X, (0, 2, 3, 1))\n",
    "\n",
    "    # get output layer Y1, Y2 from the RPN and the feature maps F\n",
    "    # Y1: y_rpn_cls\n",
    "    # Y2: y_rpn_regr\n",
    "    [Y1, Y2, F] = model_rpn.predict(X)\n",
    "\n",
    "    # Get bboxes by applying NMS \n",
    "    # R.shape = (300, 4)\n",
    "    R = rpn_to_roi(Y1, Y2, C, K.image_data_format(), overlap_thresh=0.7)\n",
    "\n",
    "    # convert from (x1,y1,x2,y2) to (x,y,w,h)\n",
    "    R[:, 2] -= R[:, 0]\n",
    "    R[:, 3] -= R[:, 1]\n",
    "\n",
    "    # apply the spatial pyramid pooling to the proposed regions\n",
    "    bboxes = {}\n",
    "    probs = {}\n",
    "\n",
    "    for jk in range(R.shape[0]//C.num_rois + 1):\n",
    "        ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n",
    "        if ROIs.shape[1] == 0:\n",
    "            break\n",
    "\n",
    "        if jk == R.shape[0]//C.num_rois:\n",
    "            #pad R\n",
    "            curr_shape = ROIs.shape\n",
    "            target_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n",
    "            ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n",
    "            ROIs_padded[:, :curr_shape[1], :] = ROIs\n",
    "            ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n",
    "            ROIs = ROIs_padded\n",
    "\n",
    "        [P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n",
    "\n",
    "        # Calculate bboxes coordinates on resized image\n",
    "        for ii in range(P_cls.shape[1]):\n",
    "            # Ignore 'bg' class\n",
    "            if np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n",
    "                continue\n",
    "\n",
    "            cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n",
    "\n",
    "            if cls_name not in bboxes:\n",
    "                bboxes[cls_name] = []\n",
    "                probs[cls_name] = []\n",
    "\n",
    "            (x, y, w, h) = ROIs[0, ii, :]\n",
    "\n",
    "            cls_num = np.argmax(P_cls[0, ii, :])\n",
    "            try:\n",
    "                (tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n",
    "                tx /= C.classifier_regr_std[0]\n",
    "                ty /= C.classifier_regr_std[1]\n",
    "                tw /= C.classifier_regr_std[2]\n",
    "                th /= C.classifier_regr_std[3]\n",
    "                x, y, w, h = apply_regr(x, y, w, h, tx, ty, tw, th)\n",
    "            except:\n",
    "                pass\n",
    "            bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n",
    "            probs[cls_name].append(np.max(P_cls[0, ii, :]))\n",
    "\n",
    "    all_dets = []\n",
    "\n",
    "    for key in bboxes:\n",
    "        bbox = np.array(bboxes[key])\n",
    "\n",
    "        new_boxes, new_probs = non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.2)\n",
    "        for jk in range(new_boxes.shape[0]):\n",
    "            (x1, y1, x2, y2) = new_boxes[jk,:]\n",
    "\n",
    "            # Calculate real coordinates on original image\n",
    "            (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)\n",
    "\n",
    "            cv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),4)\n",
    "\n",
    "            textLabel = '{}: {}'.format(key,int(100*new_probs[jk]))\n",
    "            all_dets.append((key,100*new_probs[jk]))\n",
    "\n",
    "            (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)\n",
    "            textOrg = (real_x1, real_y1-0)\n",
    "\n",
    "            cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 1)\n",
    "            cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)\n",
    "            cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)\n",
    "            b.append([idx,all_dets])\n",
    "    print('Elapsed time = {}'.format(time.time() - st))\n",
    "    print(all_dets)\n",
    "    print(\"Total no. of diseases in this leaf image\", len(all_dets))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.grid()\n",
    "    plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTbAeEjgSBG4"
   },
   "outputs": [],
   "source": [
    "# optimizer = Adam(lr=1e-5)\n",
    "# optimizer_classifier = Adam(lr=1e-5)\n",
    "# model_rpn.compile(optimizer=optimizer, loss=[rpn_loss_cls(num_anchors), rpn_loss_regr(num_anchors)])\n",
    "# model_classifier.compile(optimizer=optimizer_classifier, loss=[class_loss_cls, class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n",
    "# model_all.compile(optimizer='sgd', loss='mae')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9gIohmF0ylke",
    "ttsgdWDqaKrZ",
    "0CCVODawxRqH",
    "Dnx39FSKkPM6",
    "Cq_V3aCjcgy8",
    "df5k76UkAeQ5",
    "5VTZdK2hi5f_",
    "u4UCKfbbpzGY",
    "TgH55VPe8i55",
    "Y7S4yjj4RxlW",
    "xlMnWRTDPd7d",
    "LEvLrmtXzWZ0",
    "pEDzn17-7hbc",
    "296jBExCwBBm",
    "kfZC7LgMz9EX",
    "ys_upHart9si"
   ],
   "machine_shape": "hm",
   "name": "Plant Leaf Disease Detection and Localization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
